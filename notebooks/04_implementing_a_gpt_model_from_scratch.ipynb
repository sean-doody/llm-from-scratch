{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a GPT Model from Scratch\n",
    "\n",
    "\n",
    "This notebook covers Chapter 4 of [*Build a Large Language Model from Scratch*](https://www.manning.com/books/build-a-large-language-model-from-scratch) by Sebastian Raschka (2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding an LLM Architecture\n",
    "\n",
    "Generative pretrained transformer (GPT) models are large language models (LLMs) whose purpose is generate text, one token (or word) at a time.\n",
    "\n",
    "- While LLMs are large models, the architecture is not actually super esoteric.\n",
    "    - Many components repeat.\n",
    "\n",
    "In this notebook, we'll code te [smallest GPT-2 model](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), which has 124 million parameters. \n",
    "\n",
    "> \"In the context of deep learning and LLMs like GPT, the term 'parameters' refers to the trainable weights of the model. These weights are essentially the internal variables of the model that are adjusted and optimized during the training process to minimize a specific loss function\" (Raschka 2025:93).\n",
    "\n",
    "**A GPT Model, Visualized**\n",
    "\n",
    "![GPT Model](../img/fig-4_2-gpt-model.png)\n",
    "\n",
    "> Source: Raschka, Sebastian. 2025. *Build a Large Language Model (From Scratch).* Shelter Island, NY: Manning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by specifying the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257, # the number of unique words in the BPE tokenizer\n",
    "    \"context_length\": 1024, # the number of tokens in a sequence\n",
    "    \"emb_dim\": 768, # embedding dimensions\n",
    "    \"n_heads\": 12, # number of attention heads\n",
    "    \"n_layers\": 12, # number of layers\n",
    "    \"drop_rate\": 0.1, # the dropout rate (prevents overfitting)\n",
    "    \"qkv_bias\": False # whether to use a bias (intercept) for the Query-Key-Value linear layers\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with a basic, dummy, backbone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # token embeddings of shape (n_tokens, embedding_dims):\n",
    "        self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
    "\n",
    "        # positional embeddings of shape (context_length, embedding_dims):\n",
    "        self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
    "\n",
    "        # dropout:\n",
    "        self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
    "\n",
    "        # transformer block:\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(config)\n",
    "            for _ in range (config[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        # normalization:\n",
    "        self.final_norm = DummyLayerNorm(config[\"emb_dim\"])\n",
    "\n",
    "        # output:\n",
    "        self.out_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
    "    \n",
    "    # forward pass:\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_length = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_length, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare some input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# tokenizer:\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# sample texts:\n",
    "text1 = \"Every effort moves you\"\n",
    "text2 = \"Every day holds a\"\n",
    "\n",
    "# encode and append to batch:\n",
    "batch = []\n",
    "\n",
    "batch.append(\n",
    "    torch.tensor(tokenizer.encode(text1))\n",
    ")\n",
    "\n",
    "batch.append(\n",
    "    torch.tensor(tokenizer.encode(text2))\n",
    ")\n",
    "\n",
    "batch = torch.stack(batch, dim=0)\n",
    "\n",
    "print(f\"Batch shape: {batch.shape}\")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate a dummy model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(f\"Output shape: {logits.shape}\")\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The out tensor shape corresponds to:\n",
    "\n",
    "- `2` $\\rightarrow$ two text examples\n",
    "- `4` $\\rightarrow$ the length of the input sequences\n",
    "- `50257` $\\rightarrow$ the size of the tokenizer vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing activations with layer normalization \n",
    "\n",
    "Large, deep neural networks with many layers often encounter the issue of vanishing or exploding gradients, which makes training unstable.\n",
    "\n",
    "Layer normalization improves the stability of deep neural network training.\n",
    "\n",
    "- The goal of normalization is to obtain ***unit variance*** for the activation outputs, i.e., a mean of `0` and variance of `1`.\n",
    "- Unit variance makes converging to good weights quicker, more reliable, and more stable.\n",
    "\n",
    "In most GPT models and transformer architectures, normalization is applied before and after multi-headed attention.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get mean and variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_mean = out.mean(dim=-1, keepdim=True)\n",
    "out_var = out.var(dim=-1, keepdim=True)\n",
    "\n",
    "print(f\"Mean: {out_mean}\")\n",
    "print(f\"Variance: {out_var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_norm = (out - out_mean) / torch.sqrt(out_var)\n",
    "out_norm_mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "out_norm_var = out_norm.var(dim=-1, keepdim=True)\n",
    "\n",
    "print(f\"Normed outputs: {out_norm}\")\n",
    "print(f\"Normed means: {out_norm_mean}\")\n",
    "print(f\"Normed variances: {out_norm_var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build out the `LayerNorm` class using these insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        # small constant to prevent division by 0 errors:\n",
    "        self.eps = 1e-5\n",
    "\n",
    "        # trainable parameters to scale and shift weights\n",
    "        # if model feels that doing so will improve \n",
    "        # the training:\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False) # no Bessel correction (n-1)\n",
    "        norm_x = (x - mu) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "out_mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "out_var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "print(f\"Means: {out_mean}\")\n",
    "print(f\"Variances: {out_var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impletning a feed forward network with GELU actiations\n",
    "\n",
    "**ReLU (rectified linear unit)** has historically been the go-to activation function, but the **Gaussian error linear unit (GELU)** and **Swish-gated linear unit (SwiGLU)** are often employed in LLMs.\n",
    "\n",
    "- GELU and SwiGLU are more complicated than ReLU, but they're also smoother.\n",
    "- They also have performance enhancements over ReLU.\n",
    "\n",
    "$GELU(x) = x \\cdot \\phi(x), \\text{where} \\ \\phi \\ \\text{is the cumulative distribution function of the Gaussian distribution}$\n",
    "\n",
    "In practice, a more efficient approximation is implemented:\n",
    "\n",
    "\n",
    "$GELU(x) \\approx 0.5 \\cdot x \\cdot (1 + tanh[\\sqrt{\\frac{2}{\\pi}} \\cdot (x + 0.044715 \\cdot x^3)])$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement GELU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot GELU and ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu = GELU()\n",
    "relu = nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} Activation\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU outputs `0` if `x` is negative, else `x`.\n",
    "- Thus, `x` is always positive.\n",
    "- ReLU's sharp \"corner\" at `0` can  make optimization harder.\n",
    "\n",
    "By contrast, GELU is a smooth non-linear runction.\n",
    "- There is a non-zero gradient for most negative values (except around `-0.75`).\n",
    "- Because GELU is smoother, it can lead to better optimization during training.\n",
    "- Hence, small non-zero values are allowed.\n",
    "    - Neurons with negative values contribute to learning, though not as much as neurons with positive values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now add a **feed forward neural network** that incorporates GELU:\n",
    "\n",
    "- This network will first ***expand*** the embedding dimension by some factor (here, `4`).\n",
    "- Then, it will pass the outputs into the GELU activation function.\n",
    "- Finally, the GELU outputs are scaled back down to the original embedding dimension of `768`.\n",
    "- This expansion and contraction should allow the model to learn more complicated and nuanced representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(config[\"emb_dim\"], 4 * config[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * config[\"emb_dim\"], config[\"emb_dim\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding shortcut connections\n",
    "\n",
    "***Shortcut*** (or ***skip*** or ***residual***) connections create alternative \"shorter paths\" for gradients to pass through the network by \"skipping one or more layers\" (p. 109).\n",
    "- This helps avoid the vanishing gradient problem (e.g., gradients getting very small during repeated backpropagation).\n",
    "\n",
    "Shortcut connections work by adding input values to the outputs of a layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 layer neural network\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape: # shapes must match for shortcut\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for computing gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    # model prediction:\n",
    "    output = model(x)\n",
    "\n",
    "    # \"true\" prediction:\n",
    "    target = torch.tensor([[0.0]])\n",
    "\n",
    "    # loss:\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "\n",
    "    # backpropagation:\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"mu(abs({name})) = {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out; notice how small the gradients get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can correct for vanishing gradients by using skip connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the transformer block\n",
    "\n",
    "Key ideas:\n",
    "\n",
    "1. Multi-headed attention learns relationships between elements of the input sequence.\n",
    "2. The feed forward network \"modifies the data individually at each position\" (p. 113).\n",
    "\n",
    "The result: \"a more nuanced understanding and processing of the iinput\" and the enhancement of \"the model's overall capacity for handling complex data patterns\" (p. 113).\n",
    "\n",
    "A transformer block:\n",
    "1. Starts with inputs, whose tokens receive embeddings.\n",
    "2. Token embeddings are passed through a normalization layer.\n",
    "3. The normed outputs are passed into a masked multi-head attention layer.\n",
    "4. Dropout is applied (optionally).\n",
    "5. The outputs pass through another normalization layer.\n",
    "6. The normed outputs are passed through a feed forward network.\n",
    "    - Linear --> GELU --> Linear \n",
    "7. Drop is applied (optionally)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to fetch our `MultiHeadAttentionClass` from chapter 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # logic check:\n",
    "        assert (d_out % num_heads == 0), \"Error: d_out must be divisible by num_heads!\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # final embedding size\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # linear layer for head outputs\n",
    "        # (not strictly necessary, but commonly used):\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "\n",
    "        # dropout:\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # register buffer:\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, d_in = x.shape\n",
    "\n",
    "        # queries, keys, values\n",
    "        # of shape (batch_size, num_tokens, d_out):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        # split the matrices:\n",
    "        queries = queries.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # transpose from (batch_size, num_tokens, num_heads, head_dim)\n",
    "        # to (batch_size, num_heads, num_tokens, head_dim):\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # attention scores:\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] # mask\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # context vectors:\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) # (1, 2) --> (num_tokens, num_heads)\n",
    "        context_vec = context_vec.contiguous().view( # tensor of shape (batch_size, num_tokens, num_heads, head_dim)\n",
    "            batch_size, num_tokens, self.d_out\n",
    "        )\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can build a transformer block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # attention:\n",
    "        self.attention = MultiHeadAttention(\n",
    "            d_in=config[\"emb_dim\"],\n",
    "            d_out=config[\"emb_dim\"],\n",
    "            context_length=config[\"context_length\"],\n",
    "            num_heads=config[\"n_heads\"],\n",
    "            dropout=config[\"drop_rate\"],\n",
    "            qkv_bias=config[\"qkv_bias\"]\n",
    "        )\n",
    "\n",
    "        # feed forward:\n",
    "        self.ff = FeedForward(config)\n",
    "\n",
    "        # norm:\n",
    "        self.norm1 = LayerNorm(config[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(config[\"emb_dim\"])\n",
    "        \n",
    "        # dropout with shortcut:\n",
    "        self.drop_shortcut = nn.Dropout(config[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        # pre-layer norm:\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # attention:\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # dropout with shortcut:\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "\n",
    "        # pre-layer norm:\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # feed forward network:\n",
    "        x = self.ff(x)\n",
    "\n",
    "        # dropout with shortcut:\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding the full GPT model\n",
    "\n",
    "The basic steps of the GPT model are:\n",
    "\n",
    "1. Create token embeddings.\n",
    "2. Create positional embeddings.\n",
    "3. Add the positional embeddings to the token embeddings.\n",
    "4. Perform dropout.\n",
    "5. Pass the output from the dropout layer through `N` transformer layers.\n",
    "6. Perform a final normalization.\n",
    "7. Compute the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # token embeddings of shape (n_tokens, embedding_dims):\n",
    "        self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
    "\n",
    "        # positional embeddings of shape (context_length, embedding_dims):\n",
    "        self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
    "\n",
    "        # dropout:\n",
    "        self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
    "\n",
    "        # N transformer blocks (corresponds to n_layers):\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[\n",
    "                TransformerBlock(config) for _\n",
    "                in range(config[\"n_layers\"])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # normalization:\n",
    "        self.final_norm = LayerNorm(config[\"emb_dim\"])\n",
    "\n",
    "        # final output head:\n",
    "        self.out_head = nn.Linear(\n",
    "            config[\"emb_dim\"], config[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    # forward pass:\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        token_embeddings = self.tok_emb(in_idx)\n",
    "        pos_embeddings = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        \n",
    "        x = token_embeddings + pos_embeddings\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a GPT-2 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "output = model(batch)\n",
    "\n",
    "print(f\"Input batch:\\n {batch}\\n\")\n",
    "print(f\"Output shape: {output.shape}\\n\")\n",
    "print(f\"Output:\\n {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides functionality, like `numel()` (*number of elements*) to inspect the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of parameters for the supposed 124 million parameter model is actually >163 million. What gives?\n",
    "\n",
    "- GPT-2 uses ***weight tying***.\n",
    "- That is, weights from the token embedding layer are reused in the output layer.\n",
    "\n",
    "If we subtract the number of parameters in the output head from the total number of parameters, we'll get to 124 million parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params_gpt2 = (\n",
    "    total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    ")\n",
    "\n",
    "print(f\"Total number of trainable parameters: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters in feed forward and attention modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "total_params = sum(p.numel() for p in block.ff.parameters())\n",
    "print(f\"Feed forward parameters: {total_params}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in block.attention.parameters())\n",
    "print(f\"Attention parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume float32:\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "total_size_gb = total_size_mb / 1000\n",
    "\n",
    "print(f\"Total size: {total_size_mb:.3f} MB ({total_size_gb:.3f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text\n",
    "\n",
    "The tensor outputs from the GPT model need to converted back into tokens.\n",
    "\n",
    "- With each iteration across the input, the sequence context grows (i.e., more and more tokens are seen sequentially by the model).\n",
    "\n",
    "The shape of the output tensor is: `[batch_size, num_tokens, vocab_size]`. To get new tokens, we must:\n",
    "\n",
    "1. Decode the tensors.\n",
    "2. Select tokens based on a probability distribution.\n",
    "3. Convert tokens into natural language.\n",
    "\n",
    "In practice, the last column of the output matrix is the vector corresponding to the ***next token***.\n",
    "\n",
    "- The logits in this vector are passed through a softmax, converting them to probabilities.\n",
    "    - This isn't strictly necessary, since softmax is a monotonic transformation.\n",
    "    - That is, the index with the highest logit is also going to have the highest softmax probability.\n",
    "- The index with the highest probability corresponds the ID of the next token in the vocabulary.\n",
    "    - Simply generating the most probable next token is called ***greedy decoding***: we'll use a sampling strategy later on to make outputs more interesting.\n",
    "- The identified token is decoded and appended to the previous inputs.\n",
    "\n",
    "Implement greedy token generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        # truncate to only the last N tokens in the context_size:\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # predict:\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # focus on the last step\n",
    "        # (batch, vocab_size):\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # get probabilities:\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # get next token index with the highest probability:\n",
    "        idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "\n",
    "        # append next token to previous inputs:\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(f\"Encoded input: {start_context}\")\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(f\"Encoded tensor shape: {encoded_tensor.shape}\")\n",
    "print(f\"Encoded tensor: {encoded_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run through model:\n",
    "\n",
    "- Put the model in `.eval()` mode to disbale random components (e.g., dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model, \n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(f\"Output: {out}\")\n",
    "print(f\"Output length: {len(out[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    tokenizer.decode(out.squeeze(0).tolist())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray, we generated gibberish with an untrained model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Done!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
