{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining on Unlabeled Data\n",
    "\n",
    "This notebook covers Chapter 4 of [*Build a Large Language Model from Scratch*](https://www.manning.com/books/build-a-large-language-model-from-scratch) by Sebastian Raschka (2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load all the relevant building blocks:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "\n",
    "# tokenizer:\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# LayerNorm:\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        # small constant to prevent division by 0 errors:\n",
    "        self.eps = 1e-5\n",
    "\n",
    "        # trainable parameters to scale and shift weights\n",
    "        # if model feels that doing so will improve \n",
    "        # the training:\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False) # no Bessel correction (n-1)\n",
    "        norm_x = (x - mu) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "# GELU:\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "# feed forward:\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(config[\"emb_dim\"], 4 * config[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * config[\"emb_dim\"], config[\"emb_dim\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# multi-head attention:\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # logic check:\n",
    "        assert (d_out % num_heads == 0), \"Error: d_out must be divisible by num_heads!\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # final embedding size\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # linear layer for head outputs\n",
    "        # (not strictly necessary, but commonly used):\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "\n",
    "        # dropout:\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # register buffer:\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, d_in = x.shape\n",
    "\n",
    "        # queries, keys, values\n",
    "        # of shape (batch_size, num_tokens, d_out):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        # split the matrices:\n",
    "        queries = queries.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # transpose from (batch_size, num_tokens, num_heads, head_dim)\n",
    "        # to (batch_size, num_heads, num_tokens, head_dim):\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # attention scores:\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] # mask\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # context vectors:\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) # (1, 2) --> (num_tokens, num_heads)\n",
    "        context_vec = context_vec.contiguous().view( # tensor of shape (batch_size, num_tokens, num_heads, head_dim)\n",
    "            batch_size, num_tokens, self.d_out\n",
    "        )\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec\n",
    "\n",
    "# transformer block:\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # attention:\n",
    "        self.attention = MultiHeadAttention(\n",
    "            d_in=config[\"emb_dim\"],\n",
    "            d_out=config[\"emb_dim\"],\n",
    "            context_length=config[\"context_length\"],\n",
    "            num_heads=config[\"n_heads\"],\n",
    "            dropout=config[\"drop_rate\"],\n",
    "            qkv_bias=config[\"qkv_bias\"]\n",
    "        )\n",
    "\n",
    "        # feed forward:\n",
    "        self.ff = FeedForward(config)\n",
    "\n",
    "        # norm:\n",
    "        self.norm1 = LayerNorm(config[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(config[\"emb_dim\"])\n",
    "        \n",
    "        # dropout with shortcut:\n",
    "        self.drop_shortcut = nn.Dropout(config[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        # pre-layer norm:\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # attention:\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # dropout with shortcut:\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "\n",
    "        # pre-layer norm:\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # feed forward network:\n",
    "        x = self.ff(x)\n",
    "\n",
    "        # dropout with shortcut:\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble the GPT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # token embeddings of shape (n_tokens, embedding_dims):\n",
    "        self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
    "\n",
    "        # positional embeddings of shape (context_length, embedding_dims):\n",
    "        self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
    "\n",
    "        # dropout:\n",
    "        self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
    "\n",
    "        # N transformer blocks (corresponds to n_layers):\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[\n",
    "                TransformerBlock(config) for _\n",
    "                in range(config[\"n_layers\"])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # normalization:\n",
    "        self.final_norm = LayerNorm(config[\"emb_dim\"])\n",
    "\n",
    "        # final output head:\n",
    "        self.out_head = nn.Linear(\n",
    "            config[\"emb_dim\"], config[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    # forward pass:\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        token_embeddings = self.tok_emb(in_idx)\n",
    "        pos_embeddings = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        \n",
    "        x = token_embeddings + pos_embeddings\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the function for greedy text generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        # truncate to only the last N tokens in the context_size:\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # predict:\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # focus on the last step\n",
    "        # (batch, vocab_size):\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # get probabilities:\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # get next token index with the highest probability:\n",
    "        idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "\n",
    "        # append next token to previous inputs:\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256, # reduce for example\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two new functions for converting text to token IDs and token IDs to text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # un-squeezing adds batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # squeezing removes the batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(f\"Output text:\\n {token_ids_to_text(token_ids, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling a loss metric\n",
    "\n",
    "Clearly, the model cannot generate coherent text yet. It needs to bet rained to do so, and we need a loss metric to measure how well the model is learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have these input tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100], # --> \"every effort moves\"\n",
    "                       [40, 1107, 588]])    # --> \"I really like\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these targets:\n",
    "\n",
    "- **Note:** remember that the targets are the token IDs we want the model to produce, and correspond to the inputs shifted \"to the right\" by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([[3626, 6100, 345],   # --> \"effort moves you\"\n",
    "                        [1107, 588, 11311]]) # --> \"really like chocolate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the inputs through the model to get the logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.8849e-05, 1.5172e-05, 1.1687e-05,  ..., 2.2409e-05,\n",
      "          6.9776e-06, 1.8776e-05],\n",
      "         [9.1569e-06, 1.0062e-05, 7.8786e-06,  ..., 2.9090e-05,\n",
      "          6.0103e-06, 1.3571e-05],\n",
      "         [2.9877e-05, 8.8507e-06, 1.5741e-05,  ..., 3.5456e-05,\n",
      "          1.4094e-05, 1.3526e-05]],\n",
      "\n",
      "        [[1.2561e-05, 2.0538e-05, 1.4332e-05,  ..., 1.0389e-05,\n",
      "          3.4784e-05, 1.4239e-05],\n",
      "         [7.2731e-06, 1.7864e-05, 1.0565e-05,  ..., 2.1206e-05,\n",
      "          1.1390e-05, 1.5559e-05],\n",
      "         [2.9496e-05, 3.3605e-05, 4.1029e-05,  ..., 6.5249e-06,\n",
      "          5.8203e-05, 1.3698e-05]]])\n",
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "print(probs)\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape corresponds to:\n",
    "\n",
    "- `2` $\\rightarrow$ the batch size\n",
    "- `3` $\\rightarrow$ the number of tokens in each input\n",
    "- `50257` $\\rightarrow$ the vocabulary size\n",
    "\n",
    "How can we retrieve the relevant tokens? Use `argmax`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "print(f\"Token IDs:\\n {token_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n",
      "--------------------------------------------------\n",
      "Targets batch 2:  really like chocolate\n",
      "Outputs batch 2:  pressuring empoweredfaith\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for batch in range(len(token_ids)):\n",
    "    i = batch + 1\n",
    "    print(f\"Targets batch {i}: {token_ids_to_text(targets[batch], tokenizer)}\")\n",
    "    print(f\"Outputs batch {i}: {token_ids_to_text(token_ids[batch].flatten(), tokenizer)}\")\n",
    "    print(f\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target probs 1: tensor([7.4540e-05, 3.1061e-05, 1.1563e-05])\n",
      "------------------------------------------------------------\n",
      "Target probs 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for batch in range(len(token_ids)):\n",
    "    i = batch + 1\n",
    "    print(f\"Target probs {i}: {probs[batch, [0, 1, 2], targets[batch]]}\")\n",
    "    print(f\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure loss, we want to minimize the ***negative average log probability***:\n",
    "\n",
    "- That is, we want to bring the negative average log probability as close to `0` as possible.\n",
    "- In practice negative average log probability is often used interchangeable with ***cross-entropy loss***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative avg. log probs: 10.981441497802734\n"
     ]
    }
   ],
   "source": [
    "target_probs_1 = probs[batch, [0, 1, 2], targets[0]]\n",
    "target_probs_2 = probs[batch, [0, 1, 2], targets[1]]\n",
    "\n",
    "log_probs = torch.log(torch.cat((target_probs_1, target_probs_2)))\n",
    "avg_log_probs = torch.mean(log_probs)\n",
    "neg_avg_log_probs = avg_log_probs * -1\n",
    "\n",
    "print(f\"Negative avg. log probs: {neg_avg_log_probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply use PyTorch's `cross_entropy` method instead.\n",
    "\n",
    "Recall the shapes of our tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Targets shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to *flatten* and *combine* these tensors across the batch dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flat logits shape: torch.Size([6, 50257])\n",
      "Flat targets shape: torch.Size([6])\n",
      "\n",
      "Flat logits:\n",
      " tensor([[ 0.1113, -0.1057, -0.3666,  ...,  0.2843, -0.8824,  0.1074],\n",
      "        [-0.6109, -0.5167, -0.7613,  ...,  0.5450, -1.0319, -0.2175],\n",
      "        [ 0.5707, -0.6459, -0.0701,  ...,  0.7419, -0.1806, -0.2217],\n",
      "        [-0.2968,  0.1949, -0.1649,  ..., -0.4867,  0.7218, -0.1714],\n",
      "        [-0.8375,  0.0612, -0.4641,  ...,  0.2327, -0.3889, -0.0770],\n",
      "        [ 0.5614,  0.6919,  0.8915,  ..., -0.9472,  1.2411, -0.2056]])\n",
      "\n",
      "Flat targets:\n",
      " tensor([ 3626,  6100,   345,  1107,   588, 11311])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten(0)\n",
    "\n",
    "print(f\"Flat logits shape: {logits_flat.shape}\")\n",
    "print(f\"Flat targets shape: {targets_flat.shape}\\n\")\n",
    "\n",
    "print(f\"Flat logits:\\n {logits_flat}\\n\")\n",
    "print(f\"Flat targets:\\n {targets_flat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do cross-entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.793964385986328\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about ***perplexity***?\n",
    "\n",
    "> \"Perplexity measures howe well the probability distribution predicted by the model matches the actual distribution of the words in the dataset...Perplexity is often considered more interpretable than the raw loss value because it signifies the effective ***vocabulary size about which the model is uncertain at each step***.\" (Raschka 2025:139).\n",
    "\n",
    "- For example, a perplexity of `48725.82` would suggest the model is unsure of $~48,725$ tokens to generate next.\n",
    "\n",
    "Perplexity can be easily calculated by calling `torch.exp` on the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 48725.82\n"
     ]
    }
   ],
   "source": [
    "print(f\"Perplexity: {torch.exp(loss):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use *The Verdict* as our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "file = \"the-verdict.txt\"\n",
    "with open(file, \"r\", encoding=\"UTF-8\") as f:\n",
    "    text_data = f.read()\n",
    "\n",
    "total_chars = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(f\"Characters: {total_chars}\")\n",
    "print(f\"Tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data loader (see ch. 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# more efficient data handling:\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        \"\"\"PyTorch data loader.\n",
    "\n",
    "        Args:\n",
    "            text (str): the raw text string.\n",
    "            tokenizer (Any): the tokenizer.\n",
    "            max_length (int): the max length of the sequence.\n",
    "            stride (int): the sliding window size.\n",
    "        \"\"\"\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # encode:\n",
    "        token_ids = tokenizer.encode(text)\n",
    "\n",
    "        # get inputs and targets:\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    # __len__ gets total number of rows in dataset:\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    # __getitem__ returns a single row from the dataset:\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "    \n",
    "    \n",
    "# data loader:\n",
    "def create_dataloader_v1(text, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True, \n",
    "                         num_workers=0):\n",
    "    \n",
    "    # fetch tokenizer:\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # create dataset:\n",
    "    dataset = GPTDatasetV1(text, tokenizer, max_length, stride)\n",
    "\n",
    "    # create data loader:\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last, # if last batch is shorter than batch size, then it is dropped (this prevents loss spikes).\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the train and dev sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "train_ratio = 0.90 # 90% for training\n",
    "split_idx = int(train_ratio*len(text_data)) # index to split\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation Loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Loader:\")\n",
    "for x,y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation Loader:\")\n",
    "for x,y in val_loader:\n",
    "    print(x.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for cross-entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function to compute loss over all batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0. # cumulative loss\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches # average loss across all batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 10.988\n",
      "evaluation loss: 10.981\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    eval_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(f\"training loss: {train_loss:.3f}\")\n",
    "print(f\"evaluation loss: {eval_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, pretrain the LLM\n",
    "\n",
    "Bring it all together into a training loop.\n",
    "\n",
    "**Note:** we use the ***AdamW*** optimizer.\n",
    "\n",
    "- Adam optimizers are extremely popular.\n",
    "- AdamW is chosen over base Adam due to improved handling of weight decay.\n",
    "    - AdamW helps minimize complexity and overfitting by penalizing large weights. \n",
    "- AdmW has more effective regularization and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to contain the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluation:\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "# function for printing examples during training:\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded, max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "\n",
    "\n",
    "# function for training:\n",
    "def train_model_simple(model, train_loader, val_loader,\n",
    "                       optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    track_tokens_seen = []\n",
    "\n",
    "    tokens_seen = 0\n",
    "    global_step = -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            # reset gradients:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "\n",
    "            # loss gradients:\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights with gradients:\n",
    "            optimizer.step()\n",
    "\n",
    "            # update seen tokens:\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # eval step:\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Epoch: {epoch+1} (Step {global_step:06d})\")\n",
    "                print(f\"Training loss: {train_loss:.3f}\")\n",
    "                print(f\"Validation loss: {val_loss:.3f}\")\n",
    "                print(f\"-\"*30)\n",
    "            \n",
    "            # print example:\n",
    "            generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (Step 000000)\n",
      "Training loss: 9.783\n",
      "Validation loss: 9.927\n",
      "------------------------------\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,, the,,,,,,,,,,,,,,,,,,,,\n",
      "Every effort moves you,.                                                \n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,, the,,,,,,,,,,,,,,,,,,,,\n",
      "Every effort moves you,,,, the the, the the,,,,, the,,,, the,,,,, the the the, the,, the the the,, the the the, the the the the,,,,,\n",
      "Epoch: 1 (Step 000005)\n",
      "Training loss: 7.986\n",
      "Validation loss: 8.328\n",
      "------------------------------\n",
      "Every effort moves you, the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Every effort moves you, the,, the the the the the the the the the the the the the, the the the the the,, the the the.                     \n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Epoch: 2 (Step 000010)\n",
      "Training loss: 6.745\n",
      "Validation loss: 7.043\n",
      "------------------------------\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Every effort moves you, the, the the the, the the the, the the the the, the, the the, the the, I, the, the the, the the the, the the the the the, the the the the, the, the the\n",
      "Every effort moves you the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Every effort moves you the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Every effort moves you the the to the                                              \n",
      "Epoch: 2 (Step 000015)\n",
      "Training loss: 6.043\n",
      "Validation loss: 6.615\n",
      "------------------------------\n",
      "Every effort moves you.                                                 \n",
      "Every effort moves you, and, and. \"\". \". \". \", and, and, the, and, and, and, and, the. \". \". \", and, and, and, and\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and\n",
      "Every effort moves youNESSNESSNESSNESSききききききききききörNESSNESSNESSNESSNESSNESSNESSきき MusicNESSきききNESSNESSNESSNESSNESSNESSNESSNESSききNESSNESSききNESSきききききき\n",
      "Epoch: 3 (Step 000020)\n",
      "Training loss: 5.406\n",
      "Validation loss: 6.556\n",
      "------------------------------\n",
      "Every effort moves you of the                                                \n",
      "Every effort moves you of the to the of the of the of the of the of the of the of the \"--'s--I to the of the to the the of the the of the of the of the's of the of the \" of the's the of\n",
      "Every effort moves you of the to the picture to the picture to the of the of the of the of the picture to the of the's \" of the to the the of the the of the of the's the's of the picture and I had been to the of\n",
      "Every effort moves you, and to the picture to the picture to the picture to the picture.                                   \n",
      "Every effort moves you, and to the picture.                      \"I, and, and I had the's, and, and, and I had been, and,\n",
      "Epoch: 3 (Step 000025)\n",
      "Training loss: 5.181\n",
      "Validation loss: 6.355\n",
      "------------------------------\n",
      "Every effort moves you.                           \" I had the the--and, and I had been, and, and I had been, and I\n",
      "Every effort moves you.                           \" I had the the his I had the it. I had the--and it. I had the of\n",
      "Every effort moves you his \"              \"I was the picture--and it was the picture--and the his I had the picture-- the it the honour he had been the picture.    \n",
      "Every effort moves you his!      \"I that he had the his a and I had the picture--and it was his pictures--and it. I had the picture--and it. I had the picture and I had been he was his\n",
      "Every effort moves you, and I had been the picture.        \"I was his pictures--and, and he was his his I was his his pictures he had a of the picture, and he was his pictures, and I had\n",
      "Epoch: 4 (Step 000030)\n",
      "Training loss: 4.751\n",
      "Validation loss: 6.269\n",
      "------------------------------\n",
      "Every effort moves you, and             \"I, and I was, and, and he was, and I was his pictures, and I had been his pictures, and, and I had been, and I\n",
      "Every effort moves you, and                          \"I, and I had been, and I had been, and I had been, and I was a\n",
      "Every effort moves you know              \"I looked--and, and I had been the picture to have to have to have to have to have to the honour to the picture and I had been.  \n",
      "Every effort moves you, and I had been the picture--I to the picture.               \"I, and I had been the picture to the picture to the picture and I had been.  \n",
      "Every effort moves you, and I had been the picture--I to the, and in the, and I had been the, I had been the picture to the picture.     \"--and it, and I had the, and I had the\n",
      "Epoch: 4 (Step 000035)\n",
      "Training loss: 4.083\n",
      "Validation loss: 6.184\n",
      "------------------------------\n",
      "Every effort moves you know. \"I the picture.       \"I, and I had been the, and he had been. \"--as, and I had the picture. \"I had the, and he had been\n",
      "Every effort moves you know. \"I was a--I to the picture.   \"I looked--I--I looked, and. \"I he had the picture to see.   \"I looked and \"There.   \n",
      "Every effort moves you know it was not to the picture--I to the picture.   \"I looked--I lookedI looked, as a little of the picture--I looked he had been the picture.  \"I had been.   \n",
      "Every effort moves you know the             \"I looked--and, with the, I had been the picture.      \"I had the, the, in the picture--I was his\n",
      "Every effort moves you know the                          \"I said--as Jack's the.             \n",
      "Epoch: 5 (Step 000040)\n",
      "Training loss: 3.418\n",
      "Validation loss: 6.213\n",
      "------------------------------\n",
      "Every effort moves you know it to see that, I had been to the picture.               \"Oh, I had the picture.    \"I had the picture.    \n",
      "Every effort moves you know it to see a little of his pictures--I had the picture. Gisburn, I felt--and here are the picture to the picture, I had always to see it.            \n",
      "Every effort moves you know it's the picture--I glanced after him, I felt his last word.                   \"Oh, I had the donkey.      \n",
      "Every effort moves you know it's the picture--I glanced after him, and Mrs.              \"Oh, and I had always at my elbow and I had the donkey.      \n",
      "Every effort moves you know it was not that the picture--I he had a little the last word.           \"Oh, and I had a little.   \"I up and down the room, and in\n",
      "Epoch: 6 (Step 000045)\n",
      "Training loss: 3.019\n",
      "Validation loss: 6.082\n",
      "------------------------------\n",
      "Every effort moves you know the was not a little the of the--I had a little of a little to me--I--I looked.   \"I he had the fact-c.   \"I had a little the room, and in\n",
      "Every effort moves you know the was not that the picture--I had the fact by his last word.        \"I turned. \"--as Jack himself at my elbow and I had a little a little the room, and in\n",
      "Every effort moves you know; and in a little Mrs.     \"I turned.           \"I he had the donkey, and my elbow and I had a little a little the room, and in\n",
      "Every effort moves you know.\" \" to my dear, and he was a little the last word.        \"I turned, and he had I had always at my elbow and I had a little.      \n",
      "Every effort moves you know it was not that I felt--I had the fact that, and I had always of the fact of the fact of the donkey, and I was his pictures--that was his pictures--the, and, and down the room, I had\n",
      "Epoch: 6 (Step 000050)\n",
      "Training loss: 2.258\n",
      "Validation loss: 6.121\n",
      "------------------------------\n",
      "Every effort moves you know; and my dear Rickham--I had the fact that, I had not to my work, and to me to have to see a little of his pictures--that was his pictures--the, and, and down the room, I had\n",
      "Every effort moves you know; and my dear Rickham--I had the fact with the last word.                   \"Oh, I had the donkey. \"There were days when I\n",
      "Every effort moves you know; and my dear Rickham--I had the fact with the last word.            \"I had the head to the donkey.  \"I had a little the room, I had\n",
      "Every effort moves you know; and my dear Rickham, I had the fact with a little: \"Yes--and by me to me to have to see a little of the his head to have him--and I had the donkey, and, in a little of\n",
      "Every effort moves you know; and my dear Rickham, the deep arm-chairs forward. Gisburn--and by me to me to have to see a smile behind his painting.  \"--and I had the donkey, the first, in the first\n",
      "Epoch: 7 (Step 000055)\n",
      "Training loss: 2.012\n",
      "Validation loss: 6.215\n",
      "------------------------------\n",
      "Every effort moves you know,\" was one of the picture for nothing--I had the his last word.     \"I didn't you know, the moment--as Jack himself, the; and as his pictures--his--his, in the first\n",
      "Every effort moves you know,\" was one of the picture for nothing--I looked up his last word. Gisburn's an!  \"I looked. Gisburn's open count at my elbow and as his pictures--it was dead.\" \"I\n",
      "Every effort moves you know,\" was one of the picture for nothing--I looked up his last word. Gisburn's an!  \"I looked. Gisburn's open count at my elbow and I had the donkey. Gisburn's I had\n",
      "Every effort moves you know,\" was one of the picture for nothing--I looked up his last word. Gisburn's past!                           \n",
      "Every effort moves you know,\" was one of the picture for nothing--I looked up his last word.                                 \n",
      "Epoch: 7 (Step 000060)\n",
      "Training loss: 1.508\n",
      "Validation loss: 6.217\n",
      "------------------------------\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.           \"Oh, and I was back the head to the donkey.            \n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"Once, and moustache, I felt to see a smile behind his pictures. \"--as if he had the donkey. \"--and by his\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs. \"Oh, and--as of the fact, and to see a smile behind his pictures. \"--as if he had the donkey. \"--and by his\n",
      "Every effort moves you?\"  \"Yes--as he was to the fact with the last word.        \"I didn't--as back the head to the donkey. \"Oh, I was dead.\" \"I had\n",
      "Every effort moves you?\"  \"Yes--as he was--I told Mrs.           He laughed again, I said back the head to look up at the honour him up his pictures--because he's the first\n",
      "Epoch: 8 (Step 000065)\n",
      "Training loss: 1.184\n",
      "Validation loss: 6.248\n",
      "------------------------------\n",
      "Every effort moves you?\"     I glanced after him, one of the last word.        He laughed again, and threw back the _ my unexpected discovery; and as I turned, my eye the room, with his\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with the last word.        He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his glory, and as once one had I turned, and down the room, when I\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch: 8 (Step 000070)\n",
      "Training loss: 0.871\n",
      "Validation loss: 6.285\n",
      "------------------------------\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--she's an awful simpleton, and Mrs. I remember getting off a prodigious phrase about the honour being _mine_--because he didn't want\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--she's an awful simpleton, and Mrs. I remember getting off a prodigious phrase about the honour being _mine_--because he didn't want\n",
      "Epoch: 9 (Step 000075)\n",
      "Training loss: 0.559\n",
      "Validation loss: 6.352\n",
      "------------------------------\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the honour being _mine_--because he didn't want\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch: 9 (Step 000080)\n",
      "Training loss: 0.382\n",
      "Validation loss: 6.382\n",
      "------------------------------\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, moved aside a _jardiniere_ full of\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, moved aside a _jardiniere_ full of\n",
      "Every effort moves you?\" \" on--forming, as it were, so inevitably the background of her own picture--had lent herself in an unusual degree to the display of this false virtuosity. \"It's his ridiculous modesty, and were amusing himself by holding\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch: 10 (Step 000085)\n",
      "Training loss: 0.349\n",
      "Validation loss: 6.479\n",
      "------------------------------\n",
      "Every effort moves you?\"     I glanced after him, struck by his last word. Victor Grindle was, in fact, becoming the man of the moment--as Jack himself, one might put it, had been the man of the hour. The\n",
      "Every effort moves you?\"     I glanced after him, struck by his last word. Victor Grindle was, in fact, becoming the man of the moment--as Jack himself, one might put it, had been the man of the hour. The\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\" \"I didn't, and I haven't seen a single one in the house.\" \"Oh, and he seemed to see a smile behind his close grayish beard--as if he had the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004,\n",
    "    weight_decay=0.1\n",
    ")\n",
    "\n",
    "num_epochs = 10\n",
    "start_context = \"Every effort moves you\"\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer,\n",
    "    device, num_epochs=num_epochs, eval_freq=5,\n",
    "    eval_iter=5, start_context=start_context,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the lsos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_lsses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5,3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training Loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, ls=\"-.\", label=\"Validation Loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    \n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens Seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses)) \n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The divergence between the lines shows that the model is ***overfitting*** to the training data. It will even memorize passages from the text verbatim.\n",
    "\n",
    "- This is related to the small training data and the multiple epochs.\n",
    "- In reality, it's quite common to train on a large dataset for only a single epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding strategies to control randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask the model to generate some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " Every effort moves you?\"\n",
      "\"I didn't, and I haven't seen a single one in the house.\"\n",
      "\"Oh, and he\n"
     ]
    }
   ],
   "source": [
    "# device and eval mode:\n",
    "model.to(\"cpu\") # it should already be here anyway\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(f\"Generated text:\\n {token_ids_to_text(token_ids, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will always generate the same, most probable (i.e., \"greedy\" decoding), outcomes using the naive method. We can use ***temperature scaling*** to introduce more variety. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature Scaling\n",
    "\n",
    "An example of probabilistic sampling using a small vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token: 'forward'\n"
     ]
    }
   ],
   "source": [
    "# toy vocabulary:\n",
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2,\n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8\n",
    "}\n",
    "\n",
    "inverse_vocab = {v:k for k,v in vocab.items()}\n",
    "\n",
    "# assume our starting sequence is 'every effort moves you'\n",
    "# and it produced the following next-token logits:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "# we get the probabilities with softmax:\n",
    "probs = torch.softmax(next_token_logits, dim=0)\n",
    "\n",
    "# and the next token ID:\n",
    "next_token_id = torch.argmax(probs).item()\n",
    "print(f\"Next token: '{inverse_vocab[next_token_id]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using simply the most probable token, let's sample from a multinomial distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled token: 'toward'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probs, num_samples=1).item()\n",
    "print(f\"Sampled token: '{inverse_vocab[next_token_id]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the effect of sampling by looping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probs):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probs, num_samples=1).item()\n",
    "              for i in range(1000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "\n",
    "print_sampled_tokens(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's introduce ***temperature scaling***:\n",
    "- All that happens is the logits are divided by some number greater than $0$. \n",
    "- Temperatures greater than $1$ create \"sharper\" distributions with more \"peaks (and therefore more \"confidence\").\n",
    "    - A temperature of $1$ is as if the temperature was not applied at all.\n",
    "    - Temperatures closer to $0$ are closer to the naive argmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temperatures = [1, 0.1, 5]\n",
    "scaled_probs = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
    "\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    recs = ax.bar(x + i * bar_width, scaled_probs[i], bar_width, label=f\"Temp.: {T}\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top-k sampling\n",
    "\n",
    "Temperature scaling can sometimes result in grammatically incorrect or nonsensical outptus. To correct for this, we can use ***top-k sampling*** the restricts the sampled tokens to the top-k most likely tokens: all others are excluded.\n",
    "- All non-selected logits are replaced with $-\\infty$, resulting in probabilities of $0$ after passing through the softmax.\n",
    "- The remaining probabilities of selected tokens sum to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top token positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(f\"Top logits: {top_logits}\")\n",
    "print(f\"Top token positions: {top_pos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask out the excluded tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New logits: tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1], # lowest logit value from top-k sample\n",
    "    input=torch.tensor(float(\"-inf\")),\n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(f\"New logits: {new_logits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use softmax to convert these into probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probs = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now modify the `generate` function to take advantage of these improvements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens,\n",
    "             context_size, temperature=0.0,\n",
    "             top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        # top K:\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float(\"-inf\")).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        # temperature scaling:\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text\n",
      ": Every effort moves you know began to happen a little wild--I was such struck by his last\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(f\"Generated text\\n: {token_ids_to_text(token_ids, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading pretrained weights from OpenAI\n",
    "\n",
    "Rather than train an entire model from scratch, we can use the weights from OpenAI's GPT-2 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x7fe5cb093b50>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the GPT download script:\n",
    "import urllib.request \n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\" \n",
    "    \"LLMs-from-scratch/main/ch05/\" \n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    ") \n",
    "filename = url.split('/')[-1] \n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the script and download the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 11:11:35.011741: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743693095.055138   39253 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743693095.067508   39253 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743693095.169341   39253 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743693095.169393   39253 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743693095.169396   39253 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743693095.169398   39253 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-03 11:11:35.187936: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\", models_dir=\"gpt2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Settings: {settings}\")\n",
    "print(f\"Parameter dictionary keys: {params.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `settings` dictionary contains the LLM model settings, similar to `GPT_CONFIG_124M`. The `params` dictionary contains the model weights.\n",
    "\n",
    "We can inspect the weights, such as those for the token embeddings, contained in `wte`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding weight tensor dimensin: (50257, 768)\n",
      "Token embeddings:\n",
      " [[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token embedding weight tensor dimensin: {params['wte'].shape}\")\n",
    "print(f\"Token embeddings:\\n {params['wte']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded the weights, we can transfer them into our GPT-2 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this configuration table, we can set the model we want and update the values of `GPT_CONFIG_124M` with the corresponding parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 50257, 'context_length': 1024, 'emb_dim': 768, 'n_heads': 12, 'n_layers': 12, 'drop_rate': 0.1, 'qkv_bias': False}\n"
     ]
    }
   ],
   "source": [
    "# set the model (must be a key in `gpt2_model_configs`)\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "\n",
    "# copy our config:\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "\n",
    "# update the values:\n",
    "NEW_CONFIG.update(gpt2_model_configs[model_name])\n",
    "\n",
    "# we must also update the token length to match `1024`:\n",
    "NEW_CONFIG.update({\"context_length\": 1024})\n",
    "\n",
    "# inspect:\n",
    "print(NEW_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI also uses bias vectors in the multi-head attention linear layers, so let's update that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 50257\n",
      "context_length: 1024\n",
      "emb_dim: 768\n",
      "n_heads: 12\n",
      "n_layers: 12\n",
      "drop_rate: 0.1\n",
      "qkv_bias: True\n"
     ]
    }
   ],
   "source": [
    "# update:\n",
    "NEW_CONFIG.update({\"qkv_bias\": True})\n",
    "\n",
    "# inspect:\n",
    "for k,v in NEW_CONFIG.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the updated config to create a fresh GPT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's override the randomly generated weights with the pretrained weights from OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# helper function for assigning weights:\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch! Left: {left.shape} | Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "# function for loading the weights:\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        # divide attention and bias weights into three equal parts for Q,K,V:\n",
    "        q_w, k_w, v_w = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "\n",
    "        gpt.trf_blocks[b].attention.W_query.weight = assign(gpt.trf_blocks[b].attention.W_query.weight, q_w.T) \n",
    "        gpt.trf_blocks[b].attention.W_key.weight = assign(gpt.trf_blocks[b].attention.W_key.weight, k_w.T) \n",
    "        gpt.trf_blocks[b].attention.W_value.weight = assign(gpt.trf_blocks[b].attention.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "\n",
    "        gpt.trf_blocks[b].attention.W_query.bias = assign(gpt.trf_blocks[b].attention.W_query.bias, q_b) \n",
    "        gpt.trf_blocks[b].attention.W_key.bias = assign(gpt.trf_blocks[b].attention.W_key.bias, k_b) \n",
    "        gpt.trf_blocks[b].attention.W_value.bias = assign(gpt.trf_blocks[b].attention.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].attention.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].attention.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T\n",
    "        )\n",
    "        gpt.trf_blocks[b].attention.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].attention.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"]\n",
    "        )\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"]\n",
    "        )\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"]\n",
    "        )\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "\n",
    "    # reuse the token embeddings weights (\"weight tying\"):\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Now, let's test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " Hillary Clinton is the Democratic nominee for president in 2016, Donald Trump is already on board\" – not necessarily at the end of a campaign to become the\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Hillary Clinton is the Democratic\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(f\"Generated text:\\n {token_ids_to_text(token_ids, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the weights appears to have worked: we get coherent (albeit imperfect) text!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
