{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Attention Mechanisms\n",
    "\n",
    "This notebook covers Chapter 3 of [*Build a Large Language Model from Scratch*](https://www.manning.com/books/build-a-large-language-model-from-scratch) by Sebastian Raschka (2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem of Modeling Long Sequences\n",
    "\n",
    "- Before transformers, recurrent neural networks (RNNs) were often used for tasks like language translation.\n",
    "    - RNNs are good for sequences, since they use outputs of previous steps as inputs to \"current\" step.\n",
    "        - RNNs have an encoder and a decoder.\n",
    "        - The encoder processes the entire input context into a hidden state (\"memory cell;\" essentially an embedding).\n",
    "        - The encoder uses the hidden state to generate the output.\n",
    "    - Problematically, the decoder has no way of accessing earlier hidden states at decoding time, relying ***solely*** on the \"current hidden state,\" which can lead to a loss of context (p. 53).\n",
    "\n",
    "### Capturing Dependencies with Attention Mechanisms\n",
    "\n",
    "> \"Although RNNs work fine for translating short sentences, they don't work well for longer texts as they don't have direct access to previous words in the input. One major shortcoming int his approach is that the RNN ***must remember the entire encoded input in a single hidden state*** before passing it to the decoder\" (Raschka 2025:54).\n",
    "\n",
    "- The [***Bahdanau attention***](https://arxiv.org/abs/1409.0473) mechanism was developed in 2014 to allow the decoder of an RNN to \"selectively access different parts of the input sequence at each decoding step\" (p. 54).\n",
    "    - The selectivity of the mechanism means that some tokens will be more important than others. \n",
    "- In 2017, the [***transformer***](https://arxiv.org/abs/1706.03762) architecture was proposed.\n",
    "    - It was discovered that RNNs are not necessary for processing language and text.\n",
    "- The transformer model uses ***self-attention***.\n",
    "    - With self-attention, the relevancy of each token in each position can be considered.\n",
    "    - Each token in an input sequence can \"attend to\" all other tokens in every other position of the same sequence.\n",
    "\n",
    "### Self-attention\n",
    "\n",
    "- The \"self\" of self-attention refers to \"the mechanisms' ability to compute attention weights by relating different positions within a single input sequence\" (p. 56).\n",
    "    - Dependencies between different tokens at different positions are learned, as are their relative importance.\n",
    "- The goal is to calculate ***context vectors*** that are essentially and enriched embedding.\n",
    "    - They are \"enriched\" because they contain information about all the other tokens in the sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's start with a simplified version of self-attention without any trainable weights:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# assume these are our our initial (random) 3D embedding vectors\n",
    "# for the sequence \"Your journey starts with one step\":\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # x_1: Your\n",
    "     [0.55, 0.87, 0.66], # x_2: journey\n",
    "     [0.57, 0.85, 0.64], # x_3: starts\n",
    "     [0.22, 0.58, 0.33], # x_4: with\n",
    "     [0.77, 0.25, 0.10], # x_5: one\n",
    "     [0.05, 0.80, 0.55]  # X_6: step \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the ***intermediate attention scores*** between a \"query token\" (i.e., the token at the current position) and each token in the input sequence by taking the dot product of the query token $x^{(q)}$ and every single token in the input sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# assume our query token is position 1 [index 1], \"journey\":\n",
    "query = inputs[1]\n",
    "\n",
    "# initiate a tensor of shape inputs.shape[0] to store weights:\n",
    "attention_scores = torch.empty(inputs.shape[0])\n",
    "\n",
    "# compute attention scores for each (query, token) pair:\n",
    "for idx, x_i in enumerate(inputs):\n",
    "    attention_scores[idx] = torch.dot(x_i, query)\n",
    "\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product yields a scalar value, but it also is a ***measure of similarity***: \"a higher dot product indicates a greater degree of alignment or similarity between the vectors\" (p. 59)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, we noramlize attention scores so they sum to 1:**\n",
    "- This is useful for both interpretation and LLM training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:\n",
      " tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "Normalized attention scores:\n",
      " tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum of normed attention weights: 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "# simple approach:\n",
    "attention_scores_normed = attention_scores / attention_scores.sum()\n",
    "print(f\"Attention weights:\\n {attention_scores}\")\n",
    "print(f\"Normalized attention scores:\\n {attention_scores_normed}\")\n",
    "print(f\"Sum of normed attention weights: {attention_scores_normed.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In actual practice, we'd use the ***softmax*** function:\n",
    "- Softmax ensures weights are always positive.\n",
    "- Allows for interpreting weights as probabilities / relative importance.\n",
    "    - Higher weights indicate more improtance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax attention scores:\n",
      " tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "# simple softmax:\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "# apply:\n",
    "attention_scores_softmax = softmax_naive(attention_scores)\n",
    "print(f\"Softmax attention scores:\\n {attention_scores_softmax}\")\n",
    "print(f\"Sum: {attention_scores_softmax.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to stability issues when dealing with very large or small input values (i.e., \"overflow\" and \"underflow\"), use the PyTorch softmax implementation, which is optimized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch softmax weights:\n",
      " tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "torch_softmax_attn_scores = torch.softmax(attention_scores, dim=0)\n",
    "print(f\"PyTorch softmax weights:\\n {torch_softmax_attn_scores}\")\n",
    "print(f\"Sum: {torch_softmax_attn_scores.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we calculate the context vectors,** $z^{q}$:\n",
    "- Multiply the embedded input tokens by the attention weights.\n",
    "- Then, sum the vectors.\n",
    "- The context vector is a ***weighted sum*** of all of the input vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# again, assume we are interested in the token at position 2 [index 1]:\n",
    "query = inputs[1]\n",
    "\n",
    "# placeholder for context vector:\n",
    "context_vector = torch.zeros(query.shape)\n",
    "\n",
    "# iterate and compute:\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vector += torch_softmax_attn_scores[i]*x_i\n",
    "\n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context vector has the same dimensionality (here, `3D`) as the original token embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing attention weights for all input tokens\n",
    "\n",
    "We'll generalize the attention computation to the entire sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# our toy sequence has a length of 6, so let's create an empty tensor:\n",
    "attention_scores = torch.empty(6, 6)\n",
    "\n",
    "# now, loop over the entire sequence:\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        # attention scores tor tokens (i, j):\n",
    "        attention_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "# now, we have attention vectors for each token in the sequence:\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminate the expensive `for` loop by using matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attention_scores = inputs @ inputs.T\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize with softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "Sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(attention_scores, dim=-1) # dim=-1 sets the normalization to the \"last\" dimension of the tensor.\n",
    "print(attention_scores)\n",
    "print(f\"Sums: {attention_weights.sum(dim=-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, create all the context vectors, $z^{(i)}$, again using the dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "context_vectors = attention_weights @ inputs\n",
    "print(context_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the original ***scaled dot-product attention*** from the transformers paper.\n",
    "\n",
    "There are three trainable weight matrices:\n",
    "\n",
    "1. The ***query*** matrix, $W_q$\n",
    "    - A query is the current token being interpreted by the model.\n",
    "    - The query vector is used to determine which other tokens in the sequence are relevant to understanding the meaning of the current token.\n",
    "2. The ***key*** matrix, $W_k$\n",
    "    - A key is basically an index (used for searching), and each token has a key vector.\n",
    "    - Keys are used to match the query.\n",
    "3. The ***value*** matrix, $W_v$\n",
    "    - The value vector is meant to be analogous to a `{key: value}` pair: \n",
    "        - It contains the \"representation\" of the input.\n",
    "        - The model figures out which keys are most important to the query, and obtains its value vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By way of example, let's start at position 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "# get the token embedding:\n",
    "x_2 = inputs[1]\n",
    "\n",
    "# get the embedding dimensionality:\n",
    "d_in = inputs.shape[1]\n",
    "\n",
    "# set the output embedding size (note: these are usually equal to the input dimension):\n",
    "d_out = 2\n",
    "\n",
    "# create matrices (requires_grad=False just for simplicity):\n",
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "# get the query, key, and value vectors for token 2:\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we should use matrix multiplication to perform all key and value computations at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys shape: torch.Size([6, 2])\n",
      "Values shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(f\"Keys shape: {keys.shape}\")\n",
    "print(f\"Values shape: {values.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the unnormalized attention scores for, e.g., token 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generalize the computation across all tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to create the attention weights. But instead of performing a softmax on the raw attention scores, we scale the scores by the square root of the key matrix's embedding dimensionality:\n",
    "\n",
    "- Scaling improves training performance because it helps avoid small gradients:\n",
    "\n",
    "    > \"...large dot products can result in very small gradients during backpropagation due to the softmax function applied to them. As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero\" (p. 69). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the context vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a self-attention class\n",
    "\n",
    "Such step-by-step estimations are useful for learning, but not in practice. Let's implement self-attention as a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        # query matrix:\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "        # key matrix:\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "        # value matrix:\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    # forward pass method:\n",
    "    def forward(self, x):\n",
    "        # get queries, keys, and values:\n",
    "        queries = x @ self.W_query\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        # raw attention scores ('omega'):\n",
    "        attn_scores = queries @ keys.T\n",
    "\n",
    "        # scaled attention weights:\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        # context vectors:\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's refactor the code to use `nn.Linear` layers instead of `nn.Parameter`:\n",
    "\n",
    "- When `bias=False`, `nn.Linear` essentially performs matrix multiplication.\n",
    "- It also has optimized weight initialization, which makes it more stable during training (see p. 72)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # simpler matrix math because of nn.Linear:\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # attention:\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        # context vector:\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight initialization is differnt for `nn.Linear` and `nn.Parameter`, but we can show they are similar by transferring weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v2_v1 = SelfAttention_v2(d_in, d_out)\n",
    "sa_v2_v1_cv = sa_v2_v1(inputs)\n",
    "\n",
    "sa_v1_v2 = SelfAttention_v1(d_in, d_out)\n",
    "sa_v1_v2_cv = sa_v1_v2(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5337, -0.1051],\n",
       "        [-0.5323, -0.1080],\n",
       "        [-0.5323, -0.1079],\n",
       "        [-0.5297, -0.1076],\n",
       "        [-0.5311, -0.1066],\n",
       "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.Linear:\n",
    "sa_v2_v1_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4035, 1.0391],\n",
       "        [1.4410, 1.0669],\n",
       "        [1.4391, 1.0655],\n",
       "        [1.3786, 1.0178],\n",
       "        [1.3653, 1.0086],\n",
       "        [1.4025, 1.0361]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.Parameter:\n",
    "sa_v1_v2_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reassign weights (**note:** we must transpose `nn.Parameter` objects to match `nn.Linear.weight` objects):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4035, 1.0391],\n",
       "        [1.4410, 1.0669],\n",
       "        [1.4391, 1.0655],\n",
       "        [1.3786, 1.0178],\n",
       "        [1.3653, 1.0086],\n",
       "        [1.4025, 1.0361]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v2_v1.W_query.weight = nn.Parameter(sa_v1_v2.W_query.T)\n",
    "sa_v2_v1.W_key.weight = nn.Parameter(sa_v1_v2.W_key.T)\n",
    "sa_v2_v1.W_value.weight = nn.Parameter(sa_v1_v2.W_value.T)\n",
    "\n",
    "# rerun:\n",
    "sa_v2_v1(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the same outputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head and Causal Attention\n",
    "\n",
    "***Causal***, or ***masked***, attention mechanisms introduce masking so that the model cannot see future tokens in a sequence.\n",
    "- Each predicted word should only depend on preceding tokens.\n",
    "\n",
    "***Multi-headed attention***, well, splits the attention mechanism across multiple *heads*. \n",
    "- Each *head* is supposed to learn different things from the input data.\n",
    "- The model should learn to attend to information from different \"subspaces\" at different positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Starting with masked attention:**\n",
    "- For GPT-like models, future tokens become masked.\n",
    "- In the square attention matrix, tokens above the diagonal are masked.\n",
    "- Normalization is applied to ensure each row sums to `1`.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(\n",
    "    attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    ")\n",
    "\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `tril` to mask above the diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can simply multiple this mask matrix to the attention weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must renormalize the the weights so each row sums to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sum = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sum\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simplify this by replacing values above the diagonal in the attention weight matrix with $-\\infty$, since the softmax transforms $-\\infty$ into zero:\n",
    "\n",
    "- We can cuse `torch.triu` to implement this trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, apply the softmax, this time to `dim=1` to sum all rows to `1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding dropout to masked attention:**\n",
    "\n",
    "In ***dropout***, we randomly \"drop\" hidden layer units to prevent overfitting or biasing the model to a sepcific set of hidden layer units.\n",
    "\n",
    "- Dropout is ***only*** used during training.\n",
    "\n",
    "For transformer and GPT-like models, dropout usually is applied in one of two points:\n",
    "\n",
    "1. After calculating attention weights (more common).\n",
    "2. After applying attention weights to value vectors.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) # dropout rate of 50% (in practice, more like 10% - 20%)\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the dropout to the attention weights:\n",
    "\n",
    "- **Note:** due the reduction in elements caused by dropout, the remaining elements are scaled up by a factor of $\\frac{1}{\\text{Dropout Rate}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Package a causal attention class**\n",
    "\n",
    "Let's simulate a batch by duplicating the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n"
     ]
    }
   ],
   "source": [
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build our `CausalAttention` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        # output dims:\n",
    "        self.d_out = d_out\n",
    "\n",
    "        # query, key, and value matrices:\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # dropout:\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # this buffer helps ensure our tensors are all on the same device\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    # forward pass:\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, d_in = x.shape\n",
    "        \n",
    "        # queries, keys, and values:\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # attention:\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # we transpose (1, 2), because the batch size is at index 0.\n",
    "        attn_scores.masked_fill_( # performed in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "        )\n",
    "       \n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # context vector:\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vectors shape: torch.Size([2, 6, 2])\n",
      "tensor([[[-0.2834, -0.2539],\n",
      "         [-0.3675, -0.1289],\n",
      "         [-0.3957, -0.0917],\n",
      "         [-0.3571, -0.0482],\n",
      "         [-0.3523, -0.0932],\n",
      "         [-0.3354, -0.0407]],\n",
      "\n",
      "        [[-0.2834, -0.2539],\n",
      "         [-0.3675, -0.1289],\n",
      "         [-0.3957, -0.0917],\n",
      "         [-0.3571, -0.0482],\n",
      "         [-0.3523, -0.0932],\n",
      "         [-0.3354, -0.0407]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(f\"Context vectors shape: {context_vecs.shape}\")\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multi-headed attention, we have multiple, independent attention heads with their own sets of weights.\n",
    "\n",
    "- We can implement multi-headed causal attentio by \"stacking\" multiple `CausalAttention` modules.\n",
    "- That is, we'll create several instances of `CausalAttention`.\n",
    "- The attention heads, while independent, are run in parallel.\n",
    "\n",
    "We'll have multiple query, weight, value, and context matrices.\n",
    "\n",
    "Simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, \n",
    "                 dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # create multiple heads:\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                CausalAttention(\n",
    "                    d_in, d_out, context_length, dropout, qkv_bias\n",
    "                )\n",
    "                for head in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    # forward pass:\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running on our input with `2` heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3678, -0.2396,  0.2942,  0.0256],\n",
      "         [ 0.4388, -0.1884,  0.3751,  0.1230],\n",
      "         [ 0.4649, -0.1622,  0.4020,  0.1552],\n",
      "         [ 0.4257, -0.1461,  0.3591,  0.1498],\n",
      "         [ 0.4004, -0.0560,  0.3546,  0.1504],\n",
      "         [ 0.3921, -0.0950,  0.3361,  0.1494]],\n",
      "\n",
      "        [[ 0.3678, -0.2396,  0.2942,  0.0256],\n",
      "         [ 0.4388, -0.1884,  0.3751,  0.1230],\n",
      "         [ 0.4649, -0.1622,  0.4020,  0.1552],\n",
      "         [ 0.4257, -0.1461,  0.3591,  0.1498],\n",
      "         [ 0.4004, -0.0560,  0.3546,  0.1504],\n",
      "         [ 0.3921, -0.0950,  0.3361,  0.1494]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our output dimension is `2`, and we have `2` attention heads, the final embedding dimension is `output_dim * num_heads = 4`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "print(context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `context_vecs.shape` corresponds to:\n",
    "\n",
    "- `2` $\\rightarrow$ batch size (number of texts)\n",
    "- `6` $\\rightarrow$ the number of tokens per document (context length)\n",
    "- `4` $\\rightarrow$ the final, `4D` embedding of each token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simplify the multi-head attention calculations by combining the functionality into a single `MultIheadAttention` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # logic check:\n",
    "        assert (d_out % num_heads == 0), \"Error: d_out must be divisible by num_heads!\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # final embedding size\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # linear layer for head outputs\n",
    "        # (not strictly necessary, but commonly used):\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "\n",
    "        # dropout:\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # register buffer:\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, d_in = x.shape\n",
    "\n",
    "        # queries, keys, values\n",
    "        # of shape (batch_size, num_tokens, d_out):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        # split the matrices:\n",
    "        queries = queries.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # transpose from (batch_size, num_tokens, num_heads, head_dim)\n",
    "        # to (batch_size, num_heads, num_tokens, head_dim):\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # attention scores:\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] # mask\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # context vectors:\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) # (1, 2) --> (num_tokens, num_heads)\n",
    "        context_vec = context_vec.contiguous().view( # tensor of shape (batch_size, num_tokens, num_heads, head_dim)\n",
    "            batch_size, num_tokens, self.d_out\n",
    "        )\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the shapes and transposes, play around with this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 2])\n",
      "tensor([[[-0.3536,  0.3965],\n",
      "         [-0.3021, -0.0289],\n",
      "         [-0.3015, -0.0232],\n",
      "         [-0.1353, -0.0978],\n",
      "         [-0.2052,  0.0870],\n",
      "         [-0.1542, -0.1499]],\n",
      "\n",
      "        [[-0.3536,  0.3965],\n",
      "         [-0.3021, -0.0289],\n",
      "         [-0.3015, -0.0232],\n",
      "         [-0.1353, -0.0978],\n",
      "         [-0.2052,  0.0870],\n",
      "         [-0.1542, -0.1499]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "num_tokens = 6\n",
    "token_embedding_dims = 3\n",
    "num_heads = 2\n",
    "out_dim = 2\n",
    "head_dim = out_dim // num_heads\n",
    "\n",
    "torch.manual_seed(123)\n",
    "linear = nn.Linear(token_embedding_dims, out_dim, bias=False)\n",
    "queries = linear(batch)\n",
    "\n",
    "print(queries.shape) # (batch_size, num_tokens, out_dim)\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `view` call changes the shape to `(batch_size, num_tokens, num_heads, head_dim)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 2, 1])\n",
      "torch.Size([6, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "viewed_queries = queries.view(\n",
    "    batch_size, num_tokens, num_heads, head_dim\n",
    ")\n",
    "\n",
    "print(viewed_queries.shape)\n",
    "print(viewed_queries[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the new class:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 2\n",
      "Num tokens: 6\n",
      "Input token embedding dimensions: 3\n",
      "\n",
      "Context matrix shape: torch.Size([2, 6, 2])\n",
      "Context matrix:\n",
      " tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# seed:\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# information from our batch of inputs:\n",
    "batch_size, num_tokens, d_in = batch.shape\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Num tokens: {num_tokens}\")\n",
    "print(f\"Input token embedding dimensions: {d_in}\\n\")\n",
    "\n",
    "# multi-headed attention:\n",
    "d_out = 2 # controls dimensionality of the context vectors\n",
    "num_heads = 2\n",
    "\n",
    "mha = MultiHeadAttention(\n",
    "    d_in=d_in, d_out=d_out, context_length=num_tokens,\n",
    "    dropout=0.0, num_heads=num_heads\n",
    ")\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(f\"Context matrix shape: {context_vecs.shape}\")\n",
    "print(f\"Context matrix:\\n {context_vecs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Match the smallest GPT-2 model\n",
    "\n",
    "- `12` attention heads\n",
    "- `768` dimension input and out embeddings\n",
    "- `1024` context length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings and context:\n",
    "EMBEDDING_DIM = 768\n",
    "CONTEXT_LENGTH = 1024\n",
    "\n",
    "# simulate an input sequence:\n",
    "doc = torch.randn(CONTEXT_LENGTH, EMBEDDING_DIM)\n",
    "\n",
    "# simulate a batch:\n",
    "gpt_batch = torch.stack((doc, doc), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc shape: torch.Size([1024, 768])\n",
      "Doc tensor: tensor([[-0.1606, -0.4015,  0.6957,  ...,  1.6873,  1.7270,  0.7496],\n",
      "        [ 0.7789,  0.7275, -1.2886,  ..., -0.6831, -0.2214, -1.1118],\n",
      "        [-0.5157,  0.2716, -0.9348,  ..., -1.3915, -1.0310,  1.6252],\n",
      "        ...,\n",
      "        [ 0.6208,  1.7514,  0.0539,  ...,  1.2243, -0.5144,  0.4884],\n",
      "        [-0.0042, -0.7276,  0.8973,  ...,  0.2352,  0.3677, -0.1048],\n",
      "        [ 0.2063,  0.1570,  0.0966,  ...,  0.8885, -1.0166, -1.6929]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Doc shape: {doc.shape}\")\n",
    "print(f\"Doc tensor: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([2, 1024, 768])\n",
      "Batch tensor: tensor([[[-0.1606, -0.4015,  0.6957,  ...,  1.6873,  1.7270,  0.7496],\n",
      "         [ 0.7789,  0.7275, -1.2886,  ..., -0.6831, -0.2214, -1.1118],\n",
      "         [-0.5157,  0.2716, -0.9348,  ..., -1.3915, -1.0310,  1.6252],\n",
      "         ...,\n",
      "         [ 0.6208,  1.7514,  0.0539,  ...,  1.2243, -0.5144,  0.4884],\n",
      "         [-0.0042, -0.7276,  0.8973,  ...,  0.2352,  0.3677, -0.1048],\n",
      "         [ 0.2063,  0.1570,  0.0966,  ...,  0.8885, -1.0166, -1.6929]],\n",
      "\n",
      "        [[-0.1606, -0.4015,  0.6957,  ...,  1.6873,  1.7270,  0.7496],\n",
      "         [ 0.7789,  0.7275, -1.2886,  ..., -0.6831, -0.2214, -1.1118],\n",
      "         [-0.5157,  0.2716, -0.9348,  ..., -1.3915, -1.0310,  1.6252],\n",
      "         ...,\n",
      "         [ 0.6208,  1.7514,  0.0539,  ...,  1.2243, -0.5144,  0.4884],\n",
      "         [-0.0042, -0.7276,  0.8973,  ...,  0.2352,  0.3677, -0.1048],\n",
      "         [ 0.2063,  0.1570,  0.0966,  ...,  0.8885, -1.0166, -1.6929]]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Batch shape: {gpt_batch.shape}\")\n",
    "print(f\"Batch tensor: {gpt_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context matrix shape: torch.Size([2, 1024, 768])\n",
      "First batch context matrix:\n",
      " tensor([[ 0.4348, -0.1356, -0.6276,  ..., -0.0624, -0.1139, -0.0113],\n",
      "        [ 0.2209, -0.0992, -0.2100,  ..., -0.1952, -0.0247, -0.1179],\n",
      "        [ 0.1565,  0.0024, -0.2139,  ..., -0.0586, -0.0182, -0.2590],\n",
      "        ...,\n",
      "        [ 0.0015,  0.0321,  0.0141,  ..., -0.0213, -0.0011,  0.0176],\n",
      "        [-0.0076,  0.0298,  0.0232,  ..., -0.0211,  0.0050,  0.0185],\n",
      "        [-0.0084,  0.0305,  0.0135,  ..., -0.0069, -0.0016,  0.0177]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# GPT-2 has 12 attention heads:\n",
    "NUM_HEADS = 12\n",
    "\n",
    "# To keep 768 dimensionality as final output,\n",
    "# OUT_DIM must be equal to NUM_HEADS * EMBEDDING_DIM\n",
    "OUT_DIM = NUM_HEADS * EMBEDDING_DIM\n",
    "\n",
    "# attention object:\n",
    "mha = MultiHeadAttention(\n",
    "    d_in=EMBEDDING_DIM, d_out=EMBEDDING_DIM, context_length=CONTEXT_LENGTH,\n",
    "    dropout=0.0, num_heads=NUM_HEADS, qkv_bias=False\n",
    ")\n",
    "\n",
    "# compute:\n",
    "context_vecs = mha(gpt_batch)\n",
    "\n",
    "# inspect:\n",
    "print(f\"Context matrix shape: {context_vecs.shape}\")\n",
    "print(f\"First batch context matrix:\\n {context_vecs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
