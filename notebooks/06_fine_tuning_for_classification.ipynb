{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning for Classification\n",
    "\n",
    "This notebook covers Chapter 6 of [*Build a Large Language Model from Scratch*](https://www.manning.com/books/build-a-large-language-model-from-scratch) by Sebastian Raschka (2025).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model\n",
    "\n",
    "First, we need to reassemble the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "\n",
    "# tokenizer:\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# LayerNorm:\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        # small constant to prevent division by 0 errors:\n",
    "        self.eps = 1e-5\n",
    "\n",
    "        # trainable parameters to scale and shift weights\n",
    "        # if model feels that doing so will improve \n",
    "        # the training:\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False) # no Bessel correction (n-1)\n",
    "        norm_x = (x - mu) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "# GELU:\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "# feed forward:\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(config[\"emb_dim\"], 4 * config[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * config[\"emb_dim\"], config[\"emb_dim\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# multi-head attention:\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # logic check:\n",
    "        assert (d_out % num_heads == 0), \"Error: d_out must be divisible by num_heads!\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # final embedding size\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # linear layer for head outputs\n",
    "        # (not strictly necessary, but commonly used):\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "\n",
    "        # dropout:\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # register buffer:\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, d_in = x.shape\n",
    "\n",
    "        # queries, keys, values\n",
    "        # of shape (batch_size, num_tokens, d_out):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        # split the matrices:\n",
    "        queries = queries.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # transpose from (batch_size, num_tokens, num_heads, head_dim)\n",
    "        # to (batch_size, num_heads, num_tokens, head_dim):\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # attention scores:\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] # mask\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # context vectors:\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) # (1, 2) --> (num_tokens, num_heads)\n",
    "        context_vec = context_vec.contiguous().view( # tensor of shape (batch_size, num_tokens, num_heads, head_dim)\n",
    "            batch_size, num_tokens, self.d_out\n",
    "        )\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec\n",
    "\n",
    "# transformer block:\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # attention:\n",
    "        self.attention = MultiHeadAttention(\n",
    "            d_in=config[\"emb_dim\"],\n",
    "            d_out=config[\"emb_dim\"],\n",
    "            context_length=config[\"context_length\"],\n",
    "            num_heads=config[\"n_heads\"],\n",
    "            dropout=config[\"drop_rate\"],\n",
    "            qkv_bias=config[\"qkv_bias\"]\n",
    "        )\n",
    "\n",
    "        # feed forward:\n",
    "        self.ff = FeedForward(config)\n",
    "\n",
    "        # norm:\n",
    "        self.norm1 = LayerNorm(config[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(config[\"emb_dim\"])\n",
    "        \n",
    "        # dropout with shortcut:\n",
    "        self.drop_shortcut = nn.Dropout(config[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        # pre-layer norm:\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # attention:\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # dropout with shortcut:\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "\n",
    "        # pre-layer norm:\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # feed forward network:\n",
    "        x = self.ff(x)\n",
    "\n",
    "        # dropout with shortcut:\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        return x\n",
    "\n",
    "# model class:\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # token embeddings of shape (n_tokens, embedding_dims):\n",
    "        self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
    "\n",
    "        # positional embeddings of shape (context_length, embedding_dims):\n",
    "        self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
    "\n",
    "        # dropout:\n",
    "        self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
    "\n",
    "        # N transformer blocks (corresponds to n_layers):\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[\n",
    "                TransformerBlock(config) for _\n",
    "                in range(config[\"n_layers\"])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # normalization:\n",
    "        self.final_norm = LayerNorm(config[\"emb_dim\"])\n",
    "\n",
    "        # final output head:\n",
    "        self.out_head = nn.Linear(\n",
    "            config[\"emb_dim\"], config[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    # forward pass:\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        token_embeddings = self.tok_emb(in_idx)\n",
    "        pos_embeddings = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        \n",
    "        x = token_embeddings + pos_embeddings\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for assigning pre-trained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# helper function for assigning weights:\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch! Left: {left.shape} | Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "# function for loading the weights:\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        # divide attention and bias weights into three equal parts for Q,K,V:\n",
    "        q_w, k_w, v_w = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "\n",
    "        gpt.trf_blocks[b].attention.W_query.weight = assign(gpt.trf_blocks[b].attention.W_query.weight, q_w.T) \n",
    "        gpt.trf_blocks[b].attention.W_key.weight = assign(gpt.trf_blocks[b].attention.W_key.weight, k_w.T) \n",
    "        gpt.trf_blocks[b].attention.W_value.weight = assign(gpt.trf_blocks[b].attention.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "\n",
    "        gpt.trf_blocks[b].attention.W_query.bias = assign(gpt.trf_blocks[b].attention.W_query.bias, q_b) \n",
    "        gpt.trf_blocks[b].attention.W_key.bias = assign(gpt.trf_blocks[b].attention.W_key.bias, k_b) \n",
    "        gpt.trf_blocks[b].attention.W_value.bias = assign(gpt.trf_blocks[b].attention.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].attention.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].attention.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T\n",
    "        )\n",
    "        gpt.trf_blocks[b].attention.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].attention.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"]\n",
    "        )\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"]\n",
    "        )\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"]\n",
    "        )\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "\n",
    "    # reuse the token embeddings weights (\"weight tying\"):\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text generation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        # truncate to only the last N tokens in the context_size:\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # predict:\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # focus on the last step\n",
    "        # (batch, vocab_size):\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # get probabilities:\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # get next token index with the highest probability:\n",
    "        idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "\n",
    "        # append next token to previous inputs:\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # un-squeezing adds batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # squeezing removes the batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset\n",
    "\n",
    "We'll perform text classification on text messages to determine if they are spam or not-spam (i.e., \"ham\"). \n",
    "\n",
    "**Download the data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection/SMSSpamCollection.tsv exists; skipping download.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# helper function for downloading:\n",
    "def download_and_unzip_spam_data(\n",
    "        url, zip_path, extracted_path, data_file_path\n",
    "):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} exists; skipping download.\")\n",
    "        return\n",
    "    \n",
    "    with urllib.request.urlopen(url) as res:\n",
    "        with open(zip_path, \"wb\") as f:\n",
    "            f.write(res.read())\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        zf.extractall(extracted_path)\n",
    "\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded to: {data_file_path}\")\n",
    "\n",
    "# func parameters:\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\" \n",
    "zip_path = \"sms_spam_collection.zip\" \n",
    "extracted_path = \"sms_spam_collection\" \n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "# download:\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `.tsv` file with Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "7c178ce9-e5b9-406c-8f9a-00ad620b8ccc",
       "rows": [
        [
         "0",
         "ham",
         "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..."
        ],
        [
         "1",
         "ham",
         "Ok lar... Joking wif u oni..."
        ],
        [
         "2",
         "spam",
         "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's"
        ],
        [
         "3",
         "ham",
         "U dun say so early hor... U c already then say..."
        ],
        [
         "4",
         "ham",
         "Nah I don't think he goes to usf, he lives around here though"
        ],
        [
         "5",
         "spam",
         "FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv"
        ],
        [
         "6",
         "ham",
         "Even my brother is not like to speak with me. They treat me like aids patent."
        ],
        [
         "7",
         "ham",
         "As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune"
        ],
        [
         "8",
         "spam",
         "WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only."
        ],
        [
         "9",
         "spam",
         "Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030"
        ],
        [
         "10",
         "ham",
         "I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today."
        ],
        [
         "11",
         "spam",
         "SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info"
        ],
        [
         "12",
         "spam",
         "URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18"
        ],
        [
         "13",
         "ham",
         "I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times."
        ],
        [
         "14",
         "ham",
         "I HAVE A DATE ON SUNDAY WITH WILL!!"
        ],
        [
         "15",
         "spam",
         "XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL"
        ],
        [
         "16",
         "ham",
         "Oh k...i'm watching here:)"
        ],
        [
         "17",
         "ham",
         "Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet."
        ],
        [
         "18",
         "ham",
         "Fine if thats the way u feel. Thats the way its gota b"
        ],
        [
         "19",
         "spam",
         "England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/ú1.20 POBOXox36504W45WQ 16+"
        ],
        [
         "20",
         "ham",
         "Is that seriously how you spell his name?"
        ],
        [
         "21",
         "ham",
         "I‘m going to try for 2 months ha ha only joking"
        ],
        [
         "22",
         "ham",
         "So ü pay first lar... Then when is da stock comin..."
        ],
        [
         "23",
         "ham",
         "Aft i finish my lunch then i go str down lor. Ard 3 smth lor. U finish ur lunch already?"
        ],
        [
         "24",
         "ham",
         "Ffffffffff. Alright no way I can meet up with you sooner?"
        ],
        [
         "25",
         "ham",
         "Just forced myself to eat a slice. I'm really not hungry tho. This sucks. Mark is getting worried. He knows I'm sick when I turn down pizza. Lol"
        ],
        [
         "26",
         "ham",
         "Lol your always so convincing."
        ],
        [
         "27",
         "ham",
         "Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's left over dinner ? Do you feel my Love ?"
        ],
        [
         "28",
         "ham",
         "I'm back &amp; we're packing the car now, I'll let you know if there's room"
        ],
        [
         "29",
         "ham",
         "Ahhh. Work. I vaguely remember that! What does it feel like? Lol"
        ],
        [
         "30",
         "ham",
         "Wait that's still not all that clear, were you not sure about me being sarcastic or that that's why x doesn't want to live with us"
        ],
        [
         "31",
         "ham",
         "Yeah he got in at 2 and was v apologetic. n had fallen out and she was actin like spoilt child and he got caught up in that. Till 2! But we won't go there! Not doing too badly cheers. You? "
        ],
        [
         "32",
         "ham",
         "K tell me anything about you."
        ],
        [
         "33",
         "ham",
         "For fear of fainting with the of all that housework you just did? Quick have a cuppa"
        ],
        [
         "34",
         "spam",
         "Thanks for your subscription to Ringtone UK your mobile will be charged £5/month Please confirm by replying YES or NO. If you reply NO you will not be charged"
        ],
        [
         "35",
         "ham",
         "Yup... Ok i go home look at the timings then i msg ü again... Xuhui going to learn on 2nd may too but her lesson is at 8am"
        ],
        [
         "36",
         "ham",
         "Oops, I'll let you know when my roommate's done"
        ],
        [
         "37",
         "ham",
         "I see the letter B on my car"
        ],
        [
         "38",
         "ham",
         "Anything lor... U decide..."
        ],
        [
         "39",
         "ham",
         "Hello! How's you and how did saturday go? I was just texting to see if you'd decided to do anything tomo. Not that i'm trying to invite myself or anything!"
        ],
        [
         "40",
         "ham",
         "Pls go ahead with watts. I just wanted to be sure. Do have a great weekend. Abiola"
        ],
        [
         "41",
         "ham",
         "Did I forget to tell you ? I want you , I need you, I crave you ... But most of all ... I love you my sweet Arabian steed ... Mmmmmm ... Yummy"
        ],
        [
         "42",
         "spam",
         "07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 08000930705 for delivery tomorrow"
        ],
        [
         "43",
         "ham",
         "WHO ARE YOU SEEING?"
        ],
        [
         "44",
         "ham",
         "Great! I hope you like your man well endowed. I am  &lt;#&gt;  inches..."
        ],
        [
         "45",
         "ham",
         "No calls..messages..missed calls"
        ],
        [
         "46",
         "ham",
         "Didn't you get hep b immunisation in nigeria."
        ],
        [
         "47",
         "ham",
         "Fair enough, anything going on?"
        ],
        [
         "48",
         "ham",
         "Yeah hopefully, if tyler can't do it I could maybe ask around a bit"
        ],
        [
         "49",
         "ham",
         "U don't know how stubborn I am. I didn't even want to go to the hospital. I kept telling Mark I'm not a weak sucker. Hospitals are for weak suckers."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5572
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                               Text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load tsv via csv func:\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "\n",
    "# inspect:\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Label   5572 non-null   object\n",
      " 1   Text    5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n",
      "\n",
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label\n",
      "ham     0.865937\n",
      "spam    0.134063\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# info:\n",
    "df.info()\n",
    "\n",
    "# label distribution:\n",
    "print(f\"\\n{df['Label'].value_counts()}\\n\\n{df['Label'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an ***imbalanced dataset*** where ~86.6% (N=4825) of labels are `ham` (not spam) and ~13.4% (N=747) are `spam`.\n",
    "\n",
    "For this example, we'll sample 747 from each class (i.e., under-sampling `ham`). Then we can create a balanced dataset.\n",
    "- This is not the only option with dealing with imbalanced data, but it' just used for example purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1494 entries, 4307 to 5567\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Label   1494 non-null   object\n",
      " 1   Text    1494 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 35.0+ KB\n",
      "\n",
      "Label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label\n",
      "ham     0.5\n",
      "spam    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    ham_sample = df[df[\"Label\"] == \"ham\"].sample(\n",
    "        num_spam, random_state=123\n",
    "    )\n",
    "\n",
    "    balanced_df = pd.concat([ham_sample, df.loc[df[\"Label\"]==\"spam\"]])\n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "# get balanced df:\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "\n",
    "# inspect:\n",
    "balanced_df.info()\n",
    "\n",
    "# label distribution:\n",
    "print(f\"\\n{balanced_df['Label'].value_counts()}\\n\\n{balanced_df['Label'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dummy integer values for the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1494 entries, 4307 to 5567\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Label   1494 non-null   int64 \n",
      " 1   Text    1494 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 35.0+ KB\n",
      "\n",
      "Label\n",
      "0    747\n",
      "1    747\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "# inspect:\n",
    "balanced_df.info()\n",
    "\n",
    "# label distribution:\n",
    "print(f\"\\n{balanced_df['Label'].value_counts()}\\n\\n{balanced_df['Label'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a random split:\n",
    "- 70% training, 10% validation, 20% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "      Label                                               Text\n",
      "775      0      &lt;#&gt; ISH MINUTES WAS 5 MINUTES AGO. WTF.\n",
      "952      1  Wanna have a laugh? Try CHIT-CHAT on your mobi...\n",
      "977      0                 Now? I'm going out 4 dinner soon..\n",
      "649      1  Talk sexy!! Make new friends or fall in love i...\n",
      "890      0                          Huh so late... Fr dinner?\n",
      "\n",
      "\n",
      "Validation Data:\n",
      "       Label                                               Text\n",
      "1102      0  I had askd u a question some hours before. Its...\n",
      "1191      0           I'm in solihull, | do you want anything?\n",
      "1173      1  Hi I'm sue. I am 20 years old and work as a la...\n",
      "1146      0        Don know..he is watching film in computer..\n",
      "1120      0                               Wot u up 2 u weirdo?\n",
      "\n",
      "\n",
      "Test Data:\n",
      "       Label                                               Text\n",
      "1460      0  Nobody can decide where to eat and dad wants C...\n",
      "1435      1  UR awarded a City Break and could WIN a £200 S...\n",
      "1447      1  YOU VE WON! Your 4* Costa Del Sol Holiday or £...\n",
      "1474      1  Ringtone Club: Gr8 new polys direct to your mo...\n",
      "1296      0  Just so that you know,yetunde hasn't sent mone...\n"
     ]
    }
   ],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True) # shuffles dataframe\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    # train, val, and test sets:\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "\n",
    "# create sets:\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "\n",
    "\n",
    "# inspect:\n",
    "print(f\"Train Data:\\n {train_df.sample(n=5)}\\n\\n\")\n",
    "print(f\"Validation Data:\\n {validation_df.sample(n=5)}\\n\\n\")\n",
    "print(f\"Test Data:\\n {test_df.sample(n=5)}\")\n",
    "\n",
    "# save data for reuse later:\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Data Loaders\n",
    "\n",
    "We need to create new data loaders to use our data and fine-tune our model.\n",
    "\n",
    "We now have messages of different length: how to normalize texts? There are two primary options:\n",
    "1. Truncate all messages to the length of the shortest message in the dataset or batch.\n",
    "2. Pad the messages to length of the longest message in the dataset or batch.\n",
    "\n",
    "Truncating is cheaper computationally, but results in information loss. Therefore, we'll use padding to keep the content of all messages.\n",
    "- Here, we'll use `<|endoftext|>` as the padding token.\n",
    "\n",
    "The `<|endoftext|>` token is already in the tokenizer's vocabulary at index `50256`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare a dataset class that implements padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None,\n",
    "                 pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "        # apply padding:\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] *\n",
    "            (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets we created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 120\n"
     ]
    }
   ],
   "source": [
    "# train dataset:\n",
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# get the max training length:\n",
    "max_length = train_dataset.max_length\n",
    "print(f\"Max length: {max_length}\")\n",
    "\n",
    "\n",
    "# get val and test datasets\n",
    "# and add padding with max_length\n",
    "# to match train_dataset:\n",
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",\n",
    "    max_length=max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    max_length=max_length,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create data loaders with a batch size of 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions: torch.Size([8])\n",
      "\n",
      "\n",
      "130 training batches\n",
      "18 validation batches\n",
      "37 testing batches\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# inspect:\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(f\"Input batch dimensions: {input_batch.shape}\")\n",
    "print(f\"Label batch dimensions: {target_batch.shape}\")\n",
    "print(\"\\n\")\n",
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} testing batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the GPT Model With Pretrained Weights\n",
    "\n",
    "Now that the data is prepared, we can load our GPT model and attach the pre-trained weights from OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-small (124M) config:\n",
      "\n",
      "\tvocab_size: 50257\n",
      "\tcontext_length: 1024\n",
      "\tdrop_rate: 0.0\n",
      "\tqkv_bias: True\n",
      "\temb_dim: 768\n",
      "\tn_layers: 12\n",
      "\tn_heads: 12\n"
     ]
    }
   ],
   "source": [
    "# the model we want:\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "\n",
    "gpt2_model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25}\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(gpt2_model_configs[CHOOSE_MODEL])\n",
    "\n",
    "print(f\"{CHOOSE_MODEL} config:\\n\")\n",
    "for k,v in BASE_CONFIG.items():\n",
    "    print(f\"\\t{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get weights and load them into the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 09:39:17.628995: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743773957.677226  465225 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743773957.692860  465225 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743773957.771375  465225 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743773957.771432  465225 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743773957.771435  465225 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743773957.771437  465225 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-04 09:39:17.790162: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size, models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model's text generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves forward, but it's not enough.\n",
      "\n",
      "\"I'm not going\n"
     ]
    }
   ],
   "source": [
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(INPUT_PROMPT, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model does not have the ability to classify text yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Is the follow text 'spam'? Answer 'yes' or 'no':\n",
      "    'You are a winner you have be specially selected to receive\n",
      "    $1000 cash or a $2000 reward\n",
      "\n",
      "'You are a winner you have be specially selected to receive $1000 cash or a $2000 reward\n",
      "\n",
      "'You\n"
     ]
    }
   ],
   "source": [
    "classification_prompt = \"\"\"\n",
    "Is the follow text 'spam'? Answer 'yes' or 'no':\n",
    "    'You are a winner you have be specially selected to receive\n",
    "    $1000 cash or a $2000 reward\n",
    "\"\"\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(classification_prompt, tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Classification Head\n",
    "\n",
    "The model has to be modified to perform classification. This entails replacing the original ***output layer*** that maps the hidden representation to a vocab of size 50,257 to a smaller layer that only maps to two classes, `[0, 1]`.\n",
    "\n",
    "- In this example, instead of using a single output node (which is possible since this is binary classification), the number of output nodes will match the number of classes.\n",
    "\n",
    "Additionally, because the model is already pretrained we **do not** need to fine-tune all model layers.\n",
    "- It's better to focus on the last layers closest to the output.\n",
    "\n",
    "Glimpse the model's architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to replace the `out_head`: the final linear layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# first, freeze the model, making all layers non-trainable:\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# now, replace the output head:\n",
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(\n",
    "    in_features=BASE_CONFIG[\"emb_dim\"],\n",
    "    out_features=num_classes\n",
    ")\n",
    "\n",
    "# now, we need to make the last layer trainable:\n",
    "#    -> it actually is trainable by default!\n",
    "\n",
    "# we'll also make a few additional layers accessible to\n",
    "# fine-tuning, as this can lead to better performance.\n",
    "# here, we'll allow fine-tuning on the  final transformer block\n",
    "# and the final layer norm:\n",
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, the model will still \"work\" as it perviously did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs shape: torch.Size([1, 4])\n",
      "Outputs:\n",
      " tensor([[[-1.5854,  0.9904],\n",
      "         [-3.7235,  7.4548],\n",
      "         [-2.2661,  6.6049],\n",
      "         [-3.5983,  3.9902]]])\n",
      "Outputs dimension: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "\n",
    "print(f\"Inputs: {inputs}\")\n",
    "print(f\"Inputs shape: {inputs.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "print(f\"Outputs:\\n {outputs}\")\n",
    "print(f\"Outputs dimension: {outputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are 4 2D outputs: this corresponds to the 4 input tokens. We only want 2 possible class labels.\n",
    "\n",
    "How do we do this?\n",
    "\n",
    "- Focus only on the ***last*** output token.\n",
    "    - This works because of the causal attention mask mechanism.\n",
    "    - The last token in a sequence accumulates the most information, since it can access the entire preceding context.\n",
    "- The last token has access to data from ***all previous tokens***!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Last output token: {outputs[:,-1, :]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the classification loss and accuracy\n",
    "\n",
    "How do we actually get the predicted class label? It's actually simple:\n",
    "\n",
    "1. Convert the outputs to softmax probabilities.\n",
    "2. Take the index of the highest probability value in each row vector.\n",
    "3. The index corresponds to `0=ham` and `1=spam`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "probs = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "label = torch.argmax(probs)\n",
    "print(f\"Class label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "# we could also technically simply just take the \n",
    "# argmax of the logits, since the largest logit value\n",
    "# will also correspond to the proper index:\n",
    "logits = outputs[:, -1, :]\n",
    "label = torch.argmax(logits)\n",
    "print(f\"Class label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :]\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (\n",
    "                (predicted_labels == target_batch).sum().item()\n",
    "            )\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 53.75%\n",
      "Test accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# test on a subset of batches:\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(\n",
    "    train_loader, model, device, num_batches=10\n",
    ")\n",
    "\n",
    "val_accuracy = calc_accuracy_loader(\n",
    "    val_loader, model, device, num_batches=10\n",
    ")\n",
    "\n",
    "test_accuracy = calc_accuracy_loader(\n",
    "    test_loader, model, device, num_batches=10\n",
    ")\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the accuracies hover around 50%, because the model has not been fine-tuned to the classification task.\n",
    "\n",
    "One last step remains: defining our loss function:\n",
    "- Accuracy is not differentiable. \n",
    "- Hence, we'll use cross-entropy loss as a proxy to maximize accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    \n",
    "    # get the logits of only the last batch:\n",
    "    logits = model(input_batch)[:, -1, :]\n",
    "\n",
    "    # and the loss:\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can add a loss function for our data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the initial loss on the untrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.211\n",
      "Validation loss: 2.436\n",
      "Test loss: 2.552\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(\n",
    "        train_loader, model, device, num_batches=5\n",
    "    )\n",
    "    val_loss = calc_loss_loader(\n",
    "        val_loader, model, device, num_batches=5\n",
    "    )\n",
    "    test_loss = calc_loss_loader(\n",
    "        test_loader, model, device, num_batches=5\n",
    "    )\n",
    "\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a training function\n",
    "\n",
    "Finally, we can implement the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_simple(\n",
    "        model, train_loader, val_loader, optimizer, device,\n",
    "        num_epochs, eval_freq, eval_iter):\n",
    "    # keep track of performance:\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # reset the gradients\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward() # calculate gradients\n",
    "            optimizer.step() # update model weights\n",
    "            examples_seen += input_batch.shape[0]\n",
    "            global_step += 1\n",
    "\n",
    "            # eval:\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Epoch: {epoch+1} (Step: {global_step + 1})\")\n",
    "                print(f\"Train loss: {train_loss:.3f} | Validation loss: {val_loss:.3f}\")\n",
    "            \n",
    "        # accuracy:\n",
    "        train_accuracy = calc_accuracy_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "        val_accuracy = calc_accuracy_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen\n",
    "\n",
    "\n",
    "# evaluation function:\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (Step: 1)\n",
      "Train loss: 2.153 | Validation loss: 2.859\n",
      "Epoch: 1 (Step: 51)\n",
      "Train loss: 0.631 | Validation loss: 0.618\n",
      "Epoch: 1 (Step: 101)\n",
      "Train loss: 0.498 | Validation loss: 0.561\n",
      "Training accuracy: 75.00%\n",
      "Validation accuracy: 77.50%\n",
      "Epoch: 2 (Step: 151)\n",
      "Train loss: 0.494 | Validation loss: 0.508\n",
      "Epoch: 2 (Step: 201)\n",
      "Train loss: 0.363 | Validation loss: 0.479\n",
      "Epoch: 2 (Step: 251)\n",
      "Train loss: 0.455 | Validation loss: 0.360\n",
      "Training accuracy: 70.00%\n",
      "Validation accuracy: 92.50%\n",
      "Epoch: 3 (Step: 301)\n",
      "Train loss: 0.414 | Validation loss: 0.442\n",
      "Epoch: 3 (Step: 351)\n",
      "Train loss: 0.485 | Validation loss: 0.344\n",
      "Training accuracy: 87.50%\n",
      "Validation accuracy: 90.00%\n",
      "Epoch: 4 (Step: 401)\n",
      "Train loss: 0.220 | Validation loss: 0.190\n",
      "Epoch: 4 (Step: 451)\n",
      "Train loss: 0.204 | Validation loss: 0.196\n",
      "Epoch: 4 (Step: 501)\n",
      "Train loss: 0.147 | Validation loss: 0.091\n",
      "Training accuracy: 97.50%\n",
      "Validation accuracy: 97.50%\n",
      "Epoch: 5 (Step: 551)\n",
      "Train loss: 0.123 | Validation loss: 0.078\n",
      "Epoch: 5 (Step: 601)\n",
      "Train loss: 0.054 | Validation loss: 0.090\n",
      "Training accuracy: 95.00%\n",
      "Validation accuracy: 95.00%\n",
      "Training complete! Runtime: 21.23 mins.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "torch.manual_seed(123)\n",
    "\n",
    "start_time = time.time()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "num_epochs = 5\n",
    "\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = \\\n",
    "    train_classifier_simple(\n",
    "        model, train_loader, val_loader, optimizer, device,\n",
    "        num_epochs=num_epochs, eval_freq=50, eval_iter=5\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "runtime_mins = (end_time - start_time) / 60\n",
    "print(f\"Training complete! Runtime: {runtime_mins:.2f} mins.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the training and validation loss over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plotting function:\n",
    "def plot_values(\n",
    "        epochs_seen, examples_seen, train_values, val_values,\n",
    "        label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5,3))\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(\n",
    "        epochs_seen, val_values, linestyle=\"-.\",\n",
    "        label=f\"Validation {label}\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# plot:\n",
    "epoch_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "plot_values(epoch_tensor, examples_seen_tensor, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the accuracies across epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
    "plot_values(epoch_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate performance across the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 97.12\n",
      "validation accuracy: 96.53\n",
      "Testing accuracy: 95.61\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}\")\n",
    "print(f\"validation accuracy: {val_accuracy*100:.2f}\")\n",
    "print(f\"Testing accuracy: {test_accuracy*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model as a spam classifier\n",
    "\n",
    "Now we can use the fine-tuned model to classify spam!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(\n",
    "        text, model, tokenizer, device, \n",
    "        max_length=None, pad_token_id=50256\n",
    "    ):\n",
    "    # eval mode:\n",
    "    model.eval()\n",
    "\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_length = model.pos_emb.weight.shape[0]\n",
    "    input_ids = input_ids[:min(max_length, supported_length)]\n",
    "    input_ids += [pad_token_id]*(max_length - len(input_ids))\n",
    "    \n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1, :]\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam text prediction: `spam`\n",
      "Ham text prediction: `not spam`\n"
     ]
    }
   ],
   "source": [
    "spam_text = \"You are a winner you have been specially selected to receive $1000 cash or a $2000 award.\"\n",
    "not_spam_text = \"Hey, just wanted to check if w'ere still on for dinner tonight? Let me know!\"\n",
    "\n",
    "spam_label = classify_review(\n",
    "    spam_text, model, tokenizer, \n",
    "    device, max_length=train_dataset.max_length\n",
    ")\n",
    "\n",
    "ham_label = classify_review(\n",
    "    not_spam_text, model, tokenizer, \n",
    "    device, max_length=train_dataset.max_length\n",
    ")\n",
    "\n",
    "print(f\"Spam text prediction: `{spam_label}`\")\n",
    "print(f\"Ham text prediction: `{ham_label}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked!\n",
    "\n",
    "Now, we can save our model in case we want to use it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"review_classifier.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can reload it in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_state_dict = torch.load(\"review_classifier.pth\", map_location=device)\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
