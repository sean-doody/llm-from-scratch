{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Text Data\n",
    "\n",
    "This notebook covers Chapter 2 of [*Build a Large Language Model from Scratch*](https://www.manning.com/books/build-a-large-language-model-from-scratch) by Sebastian Raschka (2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Word Embeddings\n",
    "\n",
    "> \"The concept of converting text into a vector format is often referred to as *embedding*\" (Rashcka 2025:18).\n",
    "\n",
    "- Embeddings convert non-numerical data into continuous, dense vectors in a vector space.\n",
    "- Texts can be converted to ***word***, ***sentence***, ***paragraph***, and even ***document*** embeddings.\n",
    "- These continuous, dense embedding vectors can then be processed by neural networks.\n",
    "  - Sentence and document embeddings are used for ***retrieval-augmented generation (RAG)***.\n",
    "- The embeddings representing contextually or conceptually similar documents should be closer to each other in a vector space than those representing different contexts or concepts.\n",
    "  \n",
    "**LLM Embeddings**\n",
    "- Pretrained word embedding models, like Word2Vec, can be used (or trained from scratch) to create embeddings.\n",
    "- However, large language models (LLMs) \"commonly produce their own embeddings that are part of the input layer and are updated during training\" (Rashcka 2025:20).\n",
    "  - The benefit of using an LLM model's embeddings is that they'll be optimized to the specific task and data.\n",
    "- The size of an embedding (e.g., the ***dimensionality of its hidden state***) varies.\n",
    "  - e.g., GPT-2 used 768 dimensions, GPT-3 used 12,288."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Embeddings\n",
    "\n",
    "Fetch the public domain text, *The Verdict*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urllib:\n",
    "import urllib.request\n",
    "\n",
    "# file url:\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\" \n",
    "    \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\" \n",
    "    \"the-verdict.txt\"\n",
    ")\n",
    "\n",
    "# fetch:\n",
    "file_path = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "# open:\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"N characters: {len(raw_text)}\")\n",
    "print(raw_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple tokenizer:**\n",
    "\n",
    "We can start by just splitting on whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# split on whitespace:\n",
    "tokenizer_regex = re.compile(r\"(\\s)\")\n",
    "\n",
    "# test:\n",
    "test_text = \"Hello, world! This is a test of my simple tokenizer.\"\n",
    "test_result = re.split(tokenizer_regex, test_text)\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is too naive, so we can add additional rules (e.g., splitting punctuation from tokens):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_regex = re.compile(r\"([,.!]|\\s)\")\n",
    "test_text = \"Hello, world! This is a test of my simple tokenizer.\"\n",
    "test_result = re.split(tokenizer_regex, test_text)\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove remaining white space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = [token for token in test_result if token.strip()]\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further complexity so we can tokenize the example text from *The Verdict*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_regex = re.compile(r\"([,.:;?_!\\\"()\\']|--|\\s)\")\n",
    "tokens = re.split(tokenizer_regex, raw_text)\n",
    "tokens = [token for token in tokens if token.strip()]\n",
    "print(f\"N tokens: {len(tokens)}\")\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Map tokens to token IDs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vocabulary:\n",
    "all_words = sorted(set(tokens))\n",
    "vocab_size = len(all_words)\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token IDs:\n",
    "vocab = {token:idx for idx,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert simple tokenizer to a class:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_srt = {i:tok for tok,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r\"([,.:;?_!\\\"()\\']|--|\\s)\", text)\n",
    "        preprocessed = [tok.strip() for tok in preprocessed if tok.strip()]\n",
    "        ids = [self.str_to_int[tok] for tok in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_srt[i] for i in ids])\n",
    "\n",
    "        # remove white space before punctuation:\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab=vocab)\n",
    "\n",
    "# test text:\n",
    "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "# encode:\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode:\n",
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have the issue of out of vocabulary text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fix this with **special character** tokens:\n",
    "\n",
    "- `<|unk|>` can be used for out of vocabulary words.\n",
    "- `<|endoftext|>` can be used to communicate tot he LLM that a text sequence has ended and it is unrelated to the following sequence.\n",
    "  - In training, the training examples will be concatenated together.\n",
    "  - Hence, we need to let the model know the boundaries of related tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the vocab again:\n",
    "all_tokens = sorted(list(set(tokens)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:idx for idx,token in enumerate(all_tokens)}\n",
    "print(f\"Vocab size: {len(vocab.items())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]): print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update our tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_srt = {i:tok for tok,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r\"([,.:;?_!\\\"()\\']|--|\\s)\", text)\n",
    "        preprocessed = [tok.strip() for tok in preprocessed if tok.strip()]\n",
    "\n",
    "        # flag unkown tokens:\n",
    "        preprocessed = [\n",
    "            tok if tok in self.str_to_int else \"<|unk|>\"\n",
    "            for tok in preprocessed\n",
    "        ]\n",
    "    \n",
    "\n",
    "        ids = [self.str_to_int[tok] for tok in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_srt[i] for i in ids])\n",
    "\n",
    "        # remove white space before punctuation:\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab=vocab)\n",
    "text1 = \"Hello, do you like tea?\" \n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "# join on <|endoftext|> token:\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other common special tokens include:\n",
    "\n",
    "- `[BOS]`: beginning of sequence (start of text).\n",
    "- `[EOS]`: end of sequence (end of a text).\n",
    "  - Useful when concatenating multilpe texts.\n",
    "  - Similar to `<|endoftext|>`.\n",
    "- `[PAD]`: padding (indicates padding token).\n",
    "  - Padding is used when training examples have documents of different lengths.\n",
    "  - When training on batched inputs, a mask is typically use and we \"don't attend to padded tokens\" (p. 32).\n",
    "\n",
    "**Note:** GPT models do not use `<|unk|>` tokens; instead they use a ***byte-pair encoding tokenizr*** that breaks words into sub-word units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Byte-Pair Encoding\n",
    "\n",
    "- Byte-pair encoding (BPE) was used to train LLMs like GPT-2, GPT-3, and the original ChatGPT.\n",
    "  - This BPE tokenizer has a vocabulary size of 50,257.\n",
    "  - Even though the BPE tokenizer doesn't use `<|unk|>`, the model breaks words into subword units or individual characters, avoiding out-of-vocab (OOV) errors.\n",
    "    - If an OOV token is encountered, BPE can simply represent it as sequence of subword tokens or characters.\n",
    "  - BPE fundamentally works by \"iteratively merging frequent characters into subwords and frequent subwords into words\" (p. 34).\n",
    "- We use the implementation in `tiktoken` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version \n",
    "import tiktoken \n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get GPT-2 tokenizer:\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"\"\"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\"\"\n",
    "ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode:\n",
    "decoded_strings = tokenizer.decode(ids)\n",
    "print(decoded_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling data with a sliding window:**\n",
    "\n",
    "At each step `i` in a sequence of length `t`, the decoder can only access tokens at each step in the range `t - i`.\n",
    "\n",
    "Let's see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer.encode(raw_text)\n",
    "print(len(encoded_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask 50 first tokens:\n",
    "enc_sample = encoded_text[50:]\n",
    "\n",
    "# choose context size:\n",
    "context_size = 4\n",
    "\n",
    "# input tokens:\n",
    "input_tokens = enc_sample[:context_size]\n",
    "\n",
    "# target tokens will be the inputs shifted by one position:\n",
    "target_tokens = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"Input tokens: {input_tokens}\")\n",
    "print(f\"Target tokens: {target_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    target = enc_sample[i]\n",
    "    print(context, \"---->\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    target = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make use of PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more efficient data handling:\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        \"\"\"PyTorch data loader.\n",
    "\n",
    "        Args:\n",
    "            text (str): the raw text string.\n",
    "            tokenizer (Any): the tokenizer.\n",
    "            max_length (int): the max length of the sequence.\n",
    "            stride (int): the sliding window size.\n",
    "        \"\"\"\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # encode:\n",
    "        token_ids = tokenizer.encode(text)\n",
    "\n",
    "        # get inputs and targets:\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    # __len__ gets total number of rows in dataset:\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    # __getitem__ returns a single row from the dataset:\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(text, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True, \n",
    "                         num_workers=0):\n",
    "    \n",
    "    # fetch tokenizer:\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # create dataset:\n",
    "    dataset = GPTDatasetV1(text, tokenizer, max_length, stride)\n",
    "\n",
    "    # create data loader:\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last, # if last batch is shorter than batch size, then it is dropped (this prevents loss spikes).\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What exactaly is going on with ***stride***?\n",
    "\n",
    "- Stride controls how far the input shifts after each batch.\n",
    "\n",
    "For example, with `stride=1`, the second batch would start at `367`, the second token position of the input from the first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we set, e.g., `stride=2`, then the data loader would jump two token positions to `2885`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=2, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also up the batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"Targets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Token Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Toy Example**\n",
    "\n",
    "Assume we have a vocabulary size of just `6` and we want to create `3D` embeddings. We can use the `nn.Embedding` method from `torch` to initialize random embedding weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed:\n",
    "torch.manual_seed(123)\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random values in the embedding vectors will be optimized during training.\n",
    "\n",
    "We can grab the embedding for a specific token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_index = 3\n",
    "embedding_layer(torch.tensor([token_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate some input IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, `nn.Embedding` does the same thing as passing a one-hot-encoded matrix through a `nn.Linear` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_idx = max(input_ids) + 1\n",
    "linear_output = torch.nn.Linear(num_idx, output_dim, bias=False)\n",
    "linear_output.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight matrix must be transposed to match the shape of `nn.Embedding` (note: for this example, the actual weight values will not match):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_output.weight.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
